{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training using PyTorch with SMMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_needed = True  # should only be True once\n",
    "# install_needed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing deps and restarting kernel\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (22.1.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/.local/lib/python3.8/site-packages (2.94.0)\n",
      "Requirement already satisfied: smdebug in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (1.0.12)\n",
      "Requirement already satisfied: ipywidgets in /home/ec2-user/.local/lib/python3.8/site-packages (7.7.0)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (3.19.4)\n",
      "Requirement already satisfied: attrs==20.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (4.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (1.3.4)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (1.21.42)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (1.21.2)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: pyinstrument==3.4.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from smdebug) (3.4.2)\n",
      "Requirement already satisfied: pyinstrument-cext>=0.2.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pyinstrument==3.4.2->smdebug) (0.2.4)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /home/ec2-user/.local/lib/python3.8/site-packages (from ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipywidgets) (7.32.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipywidgets) (6.5.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipywidgets) (1.0.2)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.5.2)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.42 in /home/ec2-user/.local/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.24.46)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.6.0)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.1.0)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pickleshare in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: backcall in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (59.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.22)\n",
      "Requirement already satisfied: pygments in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.17.2)\n",
      "Requirement already satisfied: jupyter-core in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (4.9.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging>=20.0->sagemaker) (3.0.6)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->sagemaker) (2021.3)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from botocore<1.25.0,>=1.24.42->boto3<2.0,>=1.20.21->sagemaker) (1.26.8)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: entrypoints in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: pyzmq>=13 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.12.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.12.0)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.3)\n",
      "Requirement already satisfied: nbconvert in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.3.0)\n",
      "Requirement already satisfied: argon2-cffi in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.1.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: bleach in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: testpath in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.9)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (0.12.0)\n",
      "Requirement already satisfied: split-folders in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (0.5.1)\n",
      "Requirement already satisfied: wget in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (3.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchvision) (4.0.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchvision) (1.21.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchvision) (2.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: torch==1.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->torchvision) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->torchvision) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->torchvision) (2.0.7)\n",
      "Add data-root and default-shm-size=10G\n",
      "Redirecting to /bin/systemctl restart docker.service\n",
      "Docker Restart\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install --upgrade pip \n",
    "    !{sys.executable} -m pip install -U sagemaker smdebug ipywidgets --user \n",
    "    !{sys.executable} -m pip install -U torchvision split-folders wget\n",
    "    !/bin/bash ./local/local_change_setting.sh\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.debugger import (\n",
    "    Rule, ProfilerRule, rule_configs, ProfilerConfig, \n",
    "    FrameworkProfile, DetailedProfilingConfig, \n",
    "    DataloaderProfilingConfig, PythonProfilingConfig)\n",
    "import time\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.94.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Getting the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the data to S3\n",
    "We are going to use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use later when we start the training job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wget\n",
    "# import tarfile\n",
    "# import shutil\n",
    "# import os\n",
    "# import glob\n",
    "# import splitfolders\n",
    "\n",
    "# def make_dir(img_path, delete=True):\n",
    "#     import shutil, os\n",
    "#     try:\n",
    "#         if not os.path.exists(img_path):\n",
    "#             os.makedirs(img_path)\n",
    "#         else:\n",
    "#             if delete:\n",
    "#                 shutil.rmtree(img_path)\n",
    "#     except OSError:\n",
    "#         print(\"Error\")\n",
    "\n",
    "# rawimg_path = 'raw_img'\n",
    "# output_path = 'data'\n",
    "# dataset_path = './dataset'\n",
    "\n",
    "# make_dir(rawimg_path)\n",
    "# make_dir(dataset_path)\n",
    "\n",
    "# if not (os.path.isfile(\"images.tar.gz\") and tarfile.is_tarfile(\"images.tar.gz\")):\n",
    "#     wget.download('https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz')\n",
    "# tar = tarfile.open(\"images.tar.gz\")\n",
    "# tar.extractall(path=rawimg_path)\n",
    "# tar.close()\n",
    "\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# def checkImage(path):\n",
    "# #     print(path)\n",
    "#     try:\n",
    "#         with open(path, 'rb') as f:\n",
    "#             data = f.read()\n",
    "#             f.seek(-2,2)\n",
    "#             value = f.read()\n",
    "\n",
    "#         encoded_img = np.frombuffer(data, dtype = np.uint8)\n",
    "#         img_cv = cv2.imdecode(encoded_img, cv2.IMREAD_COLOR)\n",
    "# #         print(img_cv)\n",
    "#         if img_cv.shape[0]>0 and value == b'\\xff\\xd9':\n",
    "#             return True\n",
    "#     except:\n",
    "#         return False\n",
    "\n",
    "# corrupt_img = ['Egyptian_Mau_14.jpg','Egyptian_Mau_139.jpg','Egyptian_Mau_145.jpg','Egyptian_Mau_156.jpg',\n",
    "#                'Egyptian_Mau_167.jpg','Egyptian_Mau_177.jpg','Egyptian_Mau_186.jpg','Egyptian_Mau_191.jpg',\n",
    "#                'Abyssinian_5.jpg','Abyssinian_34.jpg','chihuahua_121.jpg','beagle_116.jpg']\n",
    "\n",
    "# file_dir = os.path.join(rawimg_path, 'images')\n",
    "# output_dir = os.path.join(rawimg_path, output_path)\n",
    "\n",
    "# for file_path in glob.glob(file_dir + \"/*\"):\n",
    "#     filename = file_path.split(\"/\")[2]\n",
    "#     if checkImage(file_path) and filename not in corrupt_img:\n",
    "#         dir_name = filename.split(\"_\")\n",
    "#         dir_name.pop()\n",
    "#         dir_name = \"_\".join(dir_name)\n",
    "#         dir_path = os.path.join(output_dir, dir_name)\n",
    "#         make_dir(dir_path, False)\n",
    "#         target_path = os.path.join(dir_path, filename)\n",
    "#         shutil.copyfile(file_path, target_path)\n",
    "#     else:\n",
    "#         print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num = 0\n",
    "# for file_path in glob.glob(output_dir + \"/*/*\"):\n",
    "# #     print(file_path)\n",
    "#     if not checkImage(file_path):\n",
    "# #         print(file_path)\n",
    "#         num += 1\n",
    "# print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitfolders.ratio(output_dir, output=dataset_path, seed=1337, ratio=(.8, .1, .1)) # default values\n",
    "# inputs = 's3://{}/{}'.format(bucket, 'oxford_pet_dataset')\n",
    "# !aws s3 rm $inputs --quiet --recursive\n",
    "# !aws s3 cp ./dataset $inputs --quiet --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_112062/966412700.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Training script\n",
    "The script provides all the code we need for training and hosting a SageMaker model (`model_fn` function to load a model).\n",
    "The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to.\n",
    "  These artifacts are uploaded to S3 for model hosting.\n",
    "\n",
    "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's `fit()` method, the following will be set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAINING`: A string representing the path to the directory containing data in the 'training' channel.\n",
    "\n",
    "For more information about training environment variables, please visit [SageMaker Containers](https://github.com/aws/sagemaker-containers).\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance.\n",
    "\n",
    "Because the SageMaker imports the training script, you should put your training code in a main guard (``if __name__=='__main__':``) if you are using the same script to host your model as we do in this example, so that SageMaker does not inadvertently run your training code at the wrong point in execution.\n",
    "\n",
    "For example, the script run by this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pygmentize ./train_code/pytorch_mnist_smdp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'epochs': 1,\n",
    "    'batch_size':1,\n",
    "    'img_size':3300, #10000,\n",
    "    'ddp' : True,\n",
    "    'smp' : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution={}\n",
    "\n",
    "if hyperparameters['smp']:\n",
    "    distribution_hyper = {\n",
    "        \"ddp\": hyperparameters['ddp'],\n",
    "#         \"tensor_parallel_degree\": 4,\n",
    "        \"partitions\": 8,  ## pipeline_parallel_degree\n",
    "    #     \"shard_optimizer_state\": True,\n",
    "        \"prescaled_batch\": False,\n",
    "        \"fp16_params\": True,\n",
    "        \"optimize\": \"memory\", #\"speed\",\n",
    "        \"auto_partition\": True,\n",
    "    #     \"default_partition\": 0,\n",
    "        \"microbatches\":1\n",
    "    }\n",
    "    \n",
    "    hyperparameters['prescaled_batch'] = distribution_hyper['prescaled_batch']\n",
    "    \n",
    "    distribution['smdistributed'] = {\n",
    "        \"modelparallel\": {\n",
    "            \"enabled\":True,\n",
    "            \"parameters\": distribution_hyper\n",
    "        }   \n",
    "    }\n",
    "    mpioptions = \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR \"\n",
    "    mpioptions += \"-x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 \"\n",
    "    mpioptions += \"-x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\"\n",
    "\n",
    "\n",
    "    distribution[\"mpi\"]={\n",
    "        \"enabled\": True,\n",
    "        \"processes_per_host\": 8, # Pick your processes_per_host\n",
    "        \"custom_mpi_options\": mpioptions      \n",
    "    }\n",
    "else:\n",
    "    distribution = {\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution = {\"smdistributed\": {\n",
    "#                     \"dataparallel\": {\n",
    "#                             \"enabled\": True\n",
    "#                     }\n",
    "#                }\n",
    "#              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution = {\"mpi\": {\n",
    "#                     \"enabled\": True\n",
    "#                }\n",
    "#              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = 'ml.p4d.24xlarge'\n",
    "instance_type = 'local_gpu'\n",
    "instance_count = 1\n",
    "entry_point = 'main.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo rm -rf /home/ec2-user/SageMaker/temp\n",
    "!mkdir /home/ec2-user/SageMaker/temp\n",
    "!sudo rm -rf /tmp/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if instance_type =='local_gpu':\n",
    "    from sagemaker.local import LocalSession\n",
    "    from pathlib import Path\n",
    "\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}, 'container_root':'/home/ec2-user/SageMaker/temp'}\n",
    "    s3_data_path = f'file://{Path.cwd()}/dataset'\n",
    "    source_dir = f'{Path.cwd()}/code'\n",
    "    checkpoint_s3_bucket = None\n",
    "else:\n",
    "    sess = boto3.Session()\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    sm = sess.client('sagemaker')\n",
    "    s3_data_path = inputs\n",
    "    source_dir = 'code'\n",
    "    checkpoint_s3_bucket = f's3://{bucket}/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=entry_point,\n",
    "                    source_dir=source_dir,\n",
    "                    role=role,\n",
    "                    framework_version='1.10',\n",
    "                    py_version='py38',\n",
    "                    instance_count=instance_count,\n",
    "                    instance_type=instance_type,\n",
    "                    distribution=distribution,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    disable_profiler=True,\n",
    "                    debugger_hook_config=False,\n",
    "                    max_run=1*30*60,\n",
    "                    hyperparameters=hyperparameters\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo rm -rf code/core.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating wtz4xle41m-algo-1-vtt7o ... \n",
      "Creating wtz4xle41m-algo-1-vtt7o ... done\n",
      "Attaching to wtz4xle41m-algo-1-vtt7o\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m 2022-06-11 08:27:58,927 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m 2022-06-11 08:27:59,330 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m 2022-06-11 08:27:59,334 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m 2022-06-11 08:27:59,493 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m 2022-06-11 08:27:59,493 sagemaker-training-toolkit INFO     Creating SSH daemon.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m 2022-06-11 08:27:59,495 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m 2022-06-11 08:27:59,496 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1-vtt7o'] Hosts: ['algo-1-vtt7o:8'] process_per_hosts: 8 num_processes: 8\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m 2022-06-11 08:27:59,497 sagemaker-training-toolkit INFO     Network interface name: eth0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m 2022-06-11 08:27:59,578 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m Training Env:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m {\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"additional_framework_parameters\": {\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"sagemaker_mpi_enabled\": true,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"sagemaker_mpi_num_of_processes_per_host\": 8,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"sagemaker_mpi_custom_mpi_options\": \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"sagemaker_instance_type\": \"local_gpu\"\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     },\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"training\": \"/opt/ml/input/data/training\"\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     },\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"current_host\": \"algo-1-vtt7o\",\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"algo-1-vtt7o\"\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     ],\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"epochs\": 1,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"batch_size\": 1,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"img_size\": 3300,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"ddp\": true,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"smp\": true,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"prescaled_batch\": false,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"mp_parameters\": {\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m             \"ddp\": true,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m             \"partitions\": 8,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m             \"prescaled_batch\": false,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m             \"fp16_params\": true,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m             \"optimize\": \"memory\",\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m             \"auto_partition\": true,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m             \"microbatches\": 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         }\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     },\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"training\": {\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         }\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     },\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"job_name\": \"training-job-1654936076\",\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"master_hostname\": \"algo-1-vtt7o\",\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"module_dir\": \"/opt/ml/code\",\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"module_name\": \"main\",\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"num_cpus\": 64,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"num_gpus\": 8,\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"current_host\": \"algo-1-vtt7o\",\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m             \"algo-1-vtt7o\"\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m         ]\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     },\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m     \"user_entry_point\": \"main.py\"\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m }\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m Environment variables:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_HOSTS=[\"algo-1-vtt7o\"]\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_HPS={\"batch_size\":1,\"ddp\":true,\"epochs\":1,\"img_size\":3300,\"mp_parameters\":{\"auto_partition\":true,\"ddp\":true,\"fp16_params\":true,\"microbatches\":1,\"optimize\":\"memory\",\"partitions\":8,\"prescaled_batch\":false},\"prescaled_batch\":false,\"smp\":true}\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_USER_ENTRY_POINT=main.py\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"local_gpu\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-vtt7o\",\"hosts\":[\"algo-1-vtt7o\"]}\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_INPUT_DATA_CONFIG={\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_CHANNELS=[\"training\"]\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_CURRENT_HOST=algo-1-vtt7o\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_MODULE_NAME=main\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_NUM_CPUS=64\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_NUM_GPUS=8\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_MODULE_DIR=/opt/ml/code\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"local_gpu\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-vtt7o\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-vtt7o\"],\"hyperparameters\":{\"batch_size\":1,\"ddp\":true,\"epochs\":1,\"img_size\":3300,\"mp_parameters\":{\"auto_partition\":true,\"ddp\":true,\"fp16_params\":true,\"microbatches\":1,\"optimize\":\"memory\",\"partitions\":8,\"prescaled_batch\":false},\"prescaled_batch\":false,\"smp\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"training-job-1654936076\",\"log_level\":20,\"master_hostname\":\"algo-1-vtt7o\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"main\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-vtt7o\",\"hosts\":[\"algo-1-vtt7o\"]},\"user_entry_point\":\"main.py\"}\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_USER_ARGS=[\"--batch_size\",\"1\",\"--ddp\",\"True\",\"--epochs\",\"1\",\"--img_size\",\"3300\",\"--mp_parameters\",\"auto_partition=True,ddp=True,fp16_params=True,microbatches=1,optimize=memory,partitions=8,prescaled_batch=False\",\"--prescaled_batch\",\"False\",\"--smp\",\"True\"]\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_HP_BATCH_SIZE=1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_HP_IMG_SIZE=3300\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_HP_DDP=true\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_HP_SMP=true\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_HP_PRESCALED_BATCH=false\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m SM_HP_MP_PARAMETERS={\"auto_partition\":true,\"ddp\":true,\"fp16_params\":true,\"microbatches\":1,\"optimize\":\"memory\",\"partitions\":8,\"prescaled_batch\":false}\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220524-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m mpirun --host algo-1-vtt7o:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAINING -x SM_HP_EPOCHS -x SM_HP_BATCH_SIZE -x SM_HP_IMG_SIZE -x SM_HP_DDP -x SM_HP_SMP -x SM_HP_PRESCALED_BATCH -x SM_HP_MP_PARAMETERS -x PYTHONPATH /opt/conda/bin/python3.8 -m mpi4py main.py --batch_size 1 --ddp True --epochs 1 --img_size 3300 --mp_parameters auto_partition=True,ddp=True,fp16_params=True,microbatches=1,optimize=memory,partitions=8,prescaled_batch=False --prescaled_batch False --smp True\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m Data for JOB [42248,1] offset 0 Total slots allocated 8\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m  ========================   JOB MAP   ========================\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m  Data for node: algo-1-vtt7o\tNum slots: 8\tMax slots: 0\tNum procs: 8\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m  \tProcess OMPI jobid: [42248,1] App: 0 Process rank: 0 Bound: N/A\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m  \tProcess OMPI jobid: [42248,1] App: 0 Process rank: 1 Bound: N/A\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m  \tProcess OMPI jobid: [42248,1] App: 0 Process rank: 2 Bound: N/A\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m  \tProcess OMPI jobid: [42248,1] App: 0 Process rank: 3 Bound: N/A\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m  \tProcess OMPI jobid: [42248,1] App: 0 Process rank: 4 Bound: N/A\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m  \tProcess OMPI jobid: [42248,1] App: 0 Process rank: 5 Bound: N/A\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m  \tProcess OMPI jobid: [42248,1] App: 0 Process rank: 6 Bound: N/A\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m  \tProcess OMPI jobid: [42248,1] App: 0 Process rank: 7 Bound: N/A\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m  =============================================================\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:22 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:22 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:22 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:22 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:22 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:22 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:22 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:22 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:23 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:23 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:23 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:23 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:23 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:23 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:23 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:23 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:24 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:24 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:24 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:24 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:24 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:24 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:24 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:24 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:25 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:25 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:25 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:25 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:25 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:25 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:25 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:25 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:26 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:26 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:26 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:26 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:26 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:26 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:26 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:26 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:27 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.872: I smdistributed/modelparallel/torch/state_mod.py:162] [0] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 0, rdp_rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:27 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:27 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:27 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stdout>:[2022-06-11 08:28:03.872: I smdistributed/modelparallel/torch/state_mod.py:162] [1] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 0, rdp_rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.872: I smdistributed/modelparallel/torch/throttler.py:37] Using NCCL throttle limit of 1.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stdout>:[2022-06-11 08:28:03.872: I smdistributed/modelparallel/torch/state_mod.py:162] [2] Finished initializing torch distributed process groups. pp_rank: 2, tp_rank: 0, dp_rank: 0, rdp_rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stdout>:[2022-06-11 08:28:03.872: I smdistributed/modelparallel/torch/state_mod.py:162] [6] Finished initializing torch distributed process groups. pp_rank: 6, tp_rank: 0, dp_rank: 0, rdp_rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:27 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:27 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stdout>:[2022-06-11 08:28:03.881: I smdistributed/modelparallel/torch/state_mod.py:162] [4] Finished initializing torch distributed process groups. pp_rank: 4, tp_rank: 0, dp_rank: 0, rdp_rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stdout>:[2022-06-11 08:28:03.881: I smdistributed/modelparallel/torch/state_mod.py:162] [5] Finished initializing torch distributed process groups. pp_rank: 5, tp_rank: 0, dp_rank: 0, rdp_rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:27 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:27 with 8 nodes.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stdout>:[2022-06-11 08:28:03.882: I smdistributed/modelparallel/torch/state_mod.py:162] [3] Finished initializing torch distributed process groups. pp_rank: 3, tp_rank: 0, dp_rank: 0, rdp_rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stdout>:[2022-06-11 08:28:03.882: I smdistributed/modelparallel/torch/state_mod.py:162] [7] Finished initializing torch distributed process groups. pp_rank: 7, tp_rank: 0, dp_rank: 0, rdp_rank: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.940: I smdistributed/modelparallel/backend/config.py:234] Configuration parameters:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.940: I smdistributed/modelparallel/backend/config.py:237]   pipeline_parallel_degree: 8\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.940: I smdistributed/modelparallel/backend/config.py:237]   microbatches: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.940: I smdistributed/modelparallel/backend/config.py:237]   pipeline: interleaved\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.940: I smdistributed/modelparallel/backend/config.py:237]   horovod: False\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.940: I smdistributed/modelparallel/backend/config.py:237]   ddp: True\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.940: I smdistributed/modelparallel/backend/config.py:237]   tensor_parallel_degree: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.940: I smdistributed/modelparallel/backend/config.py:237]   ddp_port: None\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.941: I smdistributed/modelparallel/backend/config.py:237]   ddp_dist_backend: nccl\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.941: I smdistributed/modelparallel/backend/config.py:237]   contiguous: True\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.941: I smdistributed/modelparallel/backend/config.py:237]   placement_strategy: cluster\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.941: I smdistributed/modelparallel/backend/config.py:237]   optimize: memory\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.941: I smdistributed/modelparallel/backend/config.py:237]   default_partition: None\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.941: I smdistributed/modelparallel/backend/config.py:237]   auto_partition: True\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.941: I smdistributed/modelparallel/backend/config.py:237]   prescaled_batch: False\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.941: I smdistributed/modelparallel/backend/config.py:237]   memory_weight: 0.8\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.941: I smdistributed/modelparallel/backend/config.py:237]   active_microbatches: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.942: I smdistributed/modelparallel/backend/config.py:237]   fp16_params: True\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.942: I smdistributed/modelparallel/backend/config.py:237]   tensor_parallel_seed: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.942: I smdistributed/modelparallel/backend/config.py:237]   offload_activations: False\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.942: I smdistributed/modelparallel/backend/config.py:237]   shard_optimizer_state: False\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.942: I smdistributed/modelparallel/backend/config.py:237]   skip_tracing: False\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:03.942: I smdistributed/modelparallel/backend/config.py:237]   activation_loading_horizon: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stdout>:=> using pre-trained model 'vgg11'\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:=> using pre-trained model 'vgg11'\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stdout>:=> using pre-trained model 'vgg11'\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stdout>:=> using pre-trained model 'vgg11'\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stdout>:=> using pre-trained model 'vgg11'\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stdout>:=> using pre-trained model 'vgg11'\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stdout>:=> using pre-trained model 'vgg11'\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stdout>:=> using pre-trained model 'vgg11'\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stderr>:Downloading: \"https://download.pytorch.org/models/vgg11-8a719046.pth\" to /root/.cache/torch/hub/checkpoints/vgg11-8a719046.pth\n",
      "  0%|          | 0.00/507M [00:00<?, ?B/s]ank:0,algo-1]<stderr>:\n",
      "  1%|▏         | 6.75M/507M [00:00<00:07, 70.2MB/s]o-1]<stderr>:\n",
      "  3%|▎         | 13.4M/507M [00:00<00:08, 63.3MB/s]o-1]<stderr>:\n",
      "  4%|▍         | 19.5M/507M [00:00<00:08, 59.5MB/s]o-1]<stderr>:\n",
      "  5%|▌         | 25.6M/507M [00:00<00:08, 60.8MB/s]o-1]<stderr>:\n",
      "  6%|▋         | 31.9M/507M [00:00<00:07, 62.8MB/s]o-1]<stderr>:\n",
      "  7%|▋         | 37.9M/507M [00:00<00:08, 60.7MB/s]o-1]<stderr>:\n",
      "  9%|▊         | 43.8M/507M [00:00<00:08, 58.8MB/s]o-1]<stderr>:\n",
      " 10%|▉         | 50.0M/507M [00:00<00:07, 60.8MB/s]o-1]<stderr>:\n",
      " 11%|█▏        | 57.1M/507M [00:00<00:07, 65.0MB/s]o-1]<stderr>:\n",
      " 13%|█▎        | 64.2M/507M [00:01<00:06, 67.7MB/s]o-1]<stderr>:\n",
      " 14%|█▍        | 70.7M/507M [00:01<00:06, 66.4MB/s]o-1]<stderr>:\n",
      " 15%|█▌        | 77.0M/507M [00:01<00:07, 61.9MB/s]o-1]<stderr>:\n",
      " 16%|█▋        | 83.6M/507M [00:01<00:06, 63.7MB/s]o-1]<stderr>:\n",
      " 18%|█▊        | 90.0M/507M [00:01<00:06, 64.7MB/s]o-1]<stderr>:\n",
      " 19%|█▉        | 96.2M/507M [00:01<00:06, 61.6MB/s]o-1]<stderr>:\n",
      " 20%|██        | 103M/507M [00:01<00:06, 65.1MB/s]go-1]<stderr>:\n",
      " 22%|██▏       | 110M/507M [00:01<00:06, 64.5MB/s]go-1]<stderr>:\n",
      " 23%|██▎       | 116M/507M [00:01<00:06, 64.5MB/s]go-1]<stderr>:\n",
      " 24%|██▍       | 122M/507M [00:02<00:06, 57.7MB/s]go-1]<stderr>:\n",
      " 25%|██▌       | 128M/507M [00:02<00:07, 56.1MB/s]go-1]<stderr>:\n",
      " 26%|██▋       | 134M/507M [00:02<00:06, 59.4MB/s]go-1]<stderr>:\n",
      " 28%|██▊       | 141M/507M [00:02<00:06, 61.7MB/s]go-1]<stderr>:\n",
      " 29%|██▉       | 147M/507M [00:02<00:06, 60.2MB/s]go-1]<stderr>:\n",
      " 30%|███       | 152M/507M [00:02<00:06, 58.5MB/s]go-1]<stderr>:\n",
      " 31%|███       | 158M/507M [00:02<00:06, 53.2MB/s]go-1]<stderr>:\n",
      " 32%|███▏      | 164M/507M [00:02<00:06, 53.6MB/s]go-1]<stderr>:\n",
      " 33%|███▎      | 169M/507M [00:02<00:07, 48.2MB/s]go-1]<stderr>:\n",
      " 34%|███▍      | 174M/507M [00:03<00:08, 42.1MB/s]go-1]<stderr>:\n",
      " 35%|███▌      | 178M/507M [00:03<00:08, 42.0MB/s]go-1]<stderr>:\n",
      " 36%|███▋      | 185M/507M [00:03<00:06, 49.6MB/s]go-1]<stderr>:\n",
      " 38%|███▊      | 192M/507M [00:03<00:05, 55.9MB/s]go-1]<stderr>:\n",
      " 39%|███▉      | 197M/507M [00:03<00:05, 55.8MB/s]go-1]<stderr>:\n",
      " 40%|████      | 204M/507M [00:03<00:05, 59.5MB/s]go-1]<stderr>:\n",
      " 41%|████▏     | 210M/507M [00:03<00:05, 57.4MB/s]go-1]<stderr>:\n",
      " 43%|████▎     | 216M/507M [00:03<00:05, 58.9MB/s]go-1]<stderr>:\n",
      " 44%|████▍     | 222M/507M [00:03<00:04, 60.6MB/s]go-1]<stderr>:\n",
      " 45%|████▍     | 228M/507M [00:04<00:05, 55.4MB/s]go-1]<stderr>:\n",
      " 46%|████▌     | 233M/507M [00:04<00:05, 56.5MB/s]go-1]<stderr>:\n",
      " 47%|████▋     | 239M/507M [00:04<00:04, 57.6MB/s]go-1]<stderr>:\n",
      " 48%|████▊     | 245M/507M [00:04<00:04, 55.4MB/s]go-1]<stderr>:\n",
      " 50%|████▉     | 252M/507M [00:04<00:04, 60.3MB/s]go-1]<stderr>:\n",
      " 51%|█████     | 258M/507M [00:04<00:04, 61.5MB/s]go-1]<stderr>:\n",
      " 52%|█████▏    | 264M/507M [00:04<00:04, 51.8MB/s]go-1]<stderr>:\n",
      " 53%|█████▎    | 269M/507M [00:04<00:05, 46.1MB/s]go-1]<stderr>:\n",
      " 54%|█████▍    | 274M/507M [00:05<00:05, 43.8MB/s]go-1]<stderr>:\n",
      " 55%|█████▌    | 280M/507M [00:05<00:04, 49.1MB/s]go-1]<stderr>:\n",
      " 57%|█████▋    | 287M/507M [00:05<00:04, 55.3MB/s]go-1]<stderr>:\n",
      " 58%|█████▊    | 293M/507M [00:05<00:03, 57.0MB/s]go-1]<stderr>:\n",
      " 59%|█████▉    | 300M/507M [00:05<00:03, 61.1MB/s]go-1]<stderr>:\n",
      " 60%|██████    | 306M/507M [00:05<00:03, 55.6MB/s]go-1]<stderr>:\n",
      " 62%|██████▏   | 312M/507M [00:05<00:03, 58.1MB/s]go-1]<stderr>:\n",
      " 63%|██████▎   | 318M/507M [00:05<00:03, 54.8MB/s]go-1]<stderr>:\n",
      " 64%|██████▍   | 324M/507M [00:05<00:03, 57.4MB/s]go-1]<stderr>:\n",
      " 65%|██████▌   | 330M/507M [00:06<00:03, 54.7MB/s]go-1]<stderr>:\n",
      " 66%|██████▌   | 335M/507M [00:06<00:03, 51.1MB/s]go-1]<stderr>:\n",
      " 67%|██████▋   | 340M/507M [00:06<00:03, 46.2MB/s]go-1]<stderr>:\n",
      " 68%|██████▊   | 346M/507M [00:06<00:03, 49.1MB/s]go-1]<stderr>:\n",
      " 69%|██████▉   | 350M/507M [00:06<00:03, 47.0MB/s]go-1]<stderr>:\n",
      " 71%|███████   | 357M/507M [00:06<00:02, 53.7MB/s]go-1]<stderr>:\n",
      " 72%|███████▏  | 363M/507M [00:06<00:02, 53.8MB/s]go-1]<stderr>:\n",
      " 73%|███████▎  | 368M/507M [00:06<00:02, 48.8MB/s]go-1]<stderr>:\n",
      " 74%|███████▎  | 373M/507M [00:07<00:03, 43.7MB/s]go-1]<stderr>:\n",
      " 75%|███████▍  | 379M/507M [00:07<00:02, 49.7MB/s]go-1]<stderr>:\n",
      " 76%|███████▌  | 386M/507M [00:07<00:02, 55.1MB/s]go-1]<stderr>:\n",
      " 77%|███████▋  | 391M/507M [00:07<00:02, 55.0MB/s]go-1]<stderr>:\n",
      " 78%|███████▊  | 397M/507M [00:07<00:02, 57.1MB/s]go-1]<stderr>:\n",
      " 80%|███████▉  | 403M/507M [00:07<00:01, 56.6MB/s]go-1]<stderr>:\n",
      " 81%|████████  | 409M/507M [00:07<00:01, 57.9MB/s]go-1]<stderr>:\n",
      " 82%|████████▏ | 415M/507M [00:07<00:01, 59.1MB/s]go-1]<stderr>:\n",
      " 83%|████████▎ | 421M/507M [00:07<00:01, 61.4MB/s]go-1]<stderr>:\n",
      " 85%|████████▍ | 428M/507M [00:07<00:01, 65.5MB/s]go-1]<stderr>:\n",
      " 86%|████████▌ | 435M/507M [00:08<00:01, 52.0MB/s]go-1]<stderr>:\n",
      " 87%|████████▋ | 441M/507M [00:08<00:01, 54.9MB/s]go-1]<stderr>:\n",
      " 88%|████████▊ | 447M/507M [00:08<00:01, 57.4MB/s]go-1]<stderr>:\n",
      " 89%|████████▉ | 453M/507M [00:08<00:01, 56.1MB/s]go-1]<stderr>:\n",
      " 90%|█████████ | 459M/507M [00:08<00:00, 56.7MB/s]go-1]<stderr>:\n",
      " 92%|█████████▏| 464M/507M [00:08<00:00, 57.1MB/s]go-1]<stderr>:\n",
      " 93%|█████████▎| 470M/507M [00:08<00:00, 58.5MB/s]go-1]<stderr>:\n",
      " 94%|█████████▍| 476M/507M [00:08<00:00, 53.7MB/s]go-1]<stderr>:\n",
      " 95%|█████████▍| 481M/507M [00:08<00:00, 54.9MB/s]go-1]<stderr>:\n",
      " 96%|█████████▋| 488M/507M [00:09<00:00, 55.8MB/s]go-1]<stderr>:\n",
      " 97%|█████████▋| 493M/507M [00:09<00:00, 56.3MB/s]go-1]<stderr>:\n",
      " 98%|█████████▊| 499M/507M [00:09<00:00, 56.4MB/s]go-1]<stderr>:\n",
      "100%|█████████▉| 504M/507M [00:09<00:00, 57.0MB/s]go-1]<stderr>:\n",
      "100%|██████████| 507M/507M [00:09<00:00, 56.1MB/s]go-1]<stderr>:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:23.849: I smdistributed/modelparallel/torch/worker.py:297] Tracing on GPU. If the model parameters do not fit in a single GPU, you can set trace_device to `cpu`.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.621: I smdistributed/modelparallel/torch/model.py:506] Partition assignments:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.621: I smdistributed/modelparallel/torch/model.py:515] main: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.621: I smdistributed/modelparallel/torch/model.py:515] main/module: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.621: I smdistributed/modelparallel/torch/model.py:515] main/module/module: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.621: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.621: I smdistributed/modelparallel/torch/model.py:515] main/module/module/avgpool: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.621: I smdistributed/modelparallel/torch/model.py:515] main/module/module/classifier: 4\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.622: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/0: 1\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.622: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/1: 2\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.622: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/2: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.622: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/3: 6\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.622: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/4: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.622: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/5: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.622: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/6: 3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.622: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/7: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.622: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/8: 7\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.623: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/9: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stdout>:[2022-06-11 08:28:27.623: I smdistributed/modelparallel/torch/model.py:440] Number of parameters on partition 2 are 0. 0 require grads\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.623: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/10: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.623: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/11: 0\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.623: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/12: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.623: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/13: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stdout>:[2022-06-11 08:28:27.623: I smdistributed/modelparallel/torch/model.py:440] Number of parameters on partition 3 are 2. 2 require grads\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stdout>:[2022-06-11 08:28:27.623: I smdistributed/modelparallel/torch/model.py:440] Number of parameters on partition 6 are 2. 2 require grads\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.623: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/14: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stdout>:[2022-06-11 08:28:27.623: I smdistributed/modelparallel/torch/model.py:440] Number of parameters on partition 1 are 2. 2 require grads\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.623: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/15: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.623: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/16: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.624: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/17: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.624: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/18: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.624: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/19: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:27.624: I smdistributed/modelparallel/torch/model.py:515] main/module/module/features/20: 5\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stdout>:[2022-06-11 08:28:27.691: I smdistributed/modelparallel/torch/model.py:440] Number of parameters on partition 7 are 2. 2 require grads\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stdout>:[2022-06-11 08:28:27.778: I smdistributed/modelparallel/torch/model.py:440] Number of parameters on partition 5 are 6. 6 require grads\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stdout>:[2022-06-11 08:28:27.938: I smdistributed/modelparallel/torch/model.py:440] Number of parameters on partition 4 are 8. 8 require grads\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:28.044: I smdistributed/modelparallel/torch/model.py:440] Number of parameters on partition 0 are 2. 2 require grads\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:28:28.045: I smdistributed/modelparallel/torch/model.py:561] Finished partitioning the model\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stdout>:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-vtt7o:38:38 [7] find_ofi_provider:550 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stdout>:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-vtt7o:38:38 [7] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stdout>:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-vtt7o:37:37 [6] find_ofi_provider:550 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stdout>:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-vtt7o:37:37 [6] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stdout>:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-vtt7o:35:35 [4] find_ofi_provider:550 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stdout>:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-vtt7o:35:35 [4] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stdout>:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-vtt7o:34:34 [3] find_ofi_provider:550 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stdout>:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-vtt7o:34:34 [3] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stdout>:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-vtt7o:36:36 [5] find_ofi_provider:550 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stdout>:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-vtt7o:36:36 [5] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stdout>:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-vtt7o:32:32 [1] find_ofi_provider:550 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stdout>:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-vtt7o:32:32 [1] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-vtt7o:31:31 [0] find_ofi_provider:550 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-vtt7o:31:31 [0] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stdout>:[2022-06-11 08:29:14.727: I smdistributed/modelparallel/torch/ddp_model.py:631] [1] Reducer buckets have been rebuilt in this iteration.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stdout>:[2022-06-11 08:29:14.728: I smdistributed/modelparallel/torch/ddp_model.py:631] [3] Reducer buckets have been rebuilt in this iteration.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stdout>:[2022-06-11 08:29:14.734: I smdistributed/modelparallel/torch/ddp_model.py:631] [6] Reducer buckets have been rebuilt in this iteration.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stdout>:[2022-06-11 08:29:14.734: I smdistributed/modelparallel/torch/ddp_model.py:631] [5] Reducer buckets have been rebuilt in this iteration.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stdout>:[2022-06-11 08:29:14.735: I smdistributed/modelparallel/torch/ddp_model.py:631] [4] Reducer buckets have been rebuilt in this iteration.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-11 08:29:14.735: I smdistributed/modelparallel/torch/ddp_model.py:631] [0] Reducer buckets have been rebuilt in this iteration.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stdout>:[2022-06-11 08:29:14.736: I smdistributed/modelparallel/torch/ddp_model.py:631] [7] Reducer buckets have been rebuilt in this iteration.\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:1,algo-1]<stdout>:Epoch: [1][0/5899] Train_Time=5.760: avg=5.760, Train_Speed=0.174 (0.174), Train_Loss=3.6035156250:(3.6035), Train_Prec@1=0.000:(0.000), Train_Prec@5=0.000:(0.000)\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:7,algo-1]<stdout>:Epoch: [1][0/5899] Train_Time=5.761: avg=5.761, Train_Speed=0.174 (0.174), Train_Loss=3.6035156250:(3.6035), Train_Prec@1=0.000:(0.000), Train_Prec@5=0.000:(0.000)\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:6,algo-1]<stdout>:Epoch: [1][0/5899] Train_Time=5.762: avg=5.762, Train_Speed=0.174 (0.174), Train_Loss=3.6035156250:(3.6035), Train_Prec@1=0.000:(0.000), Train_Prec@5=0.000:(0.000)\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:2,algo-1]<stdout>:Epoch: [1][0/5899] Train_Time=5.763: avg=5.763, Train_Speed=0.174 (0.174), Train_Loss=3.6035156250:(3.6035), Train_Prec@1=0.000:(0.000), Train_Prec@5=0.000:(0.000)\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:3,algo-1]<stdout>:Epoch: [1][0/5899] Train_Time=5.766: avg=5.766, Train_Speed=0.173 (0.173), Train_Loss=3.6035156250:(3.6035), Train_Prec@1=0.000:(0.000), Train_Prec@5=0.000:(0.000)\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:5,algo-1]<stdout>:Epoch: [1][0/5899] Train_Time=5.767: avg=5.767, Train_Speed=0.173 (0.173), Train_Loss=3.6035156250:(3.6035), Train_Prec@1=0.000:(0.000), Train_Prec@5=0.000:(0.000)\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:4,algo-1]<stdout>:Epoch: [1][0/5899] Train_Time=5.764: avg=5.764, Train_Speed=0.173 (0.173), Train_Loss=3.6035156250:(3.6035), Train_Prec@1=0.000:(0.000), Train_Prec@5=0.000:(0.000)\n",
      "\u001b[36mwtz4xle41m-algo-1-vtt7o |\u001b[0m [1,mpirank:0,algo-1]<stdout>:Epoch: [1][0/5899] Train_Time=5.765: avg=5.765, Train_Speed=0.173 (0.173), Train_Loss=3.6035156250:(3.6035), Train_Prec@1=0.000:(0.000), Train_Prec@5=0.000:(0.000)\n"
     ]
    }
   ],
   "source": [
    "job_name = \"training-job-{}\".format(int(time.time()))\n",
    "\n",
    "estimator.fit({'training': s3_data_path},\n",
    "              job_name=job_name,\n",
    "              wait=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.logs_for_job(job_name=job_name, wait=True)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
