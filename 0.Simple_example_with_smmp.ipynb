{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training using PyTorch with SMMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_needed = True  # should only be True once\n",
    "# install_needed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing deps and restarting kernel\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (22.1.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/.local/lib/python3.8/site-packages (2.94.0)\n",
      "Requirement already satisfied: smdebug in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (1.0.12)\n",
      "Requirement already satisfied: ipywidgets in /home/ec2-user/.local/lib/python3.8/site-packages (7.7.0)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (3.19.4)\n",
      "Requirement already satisfied: attrs==20.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (4.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (1.3.4)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (1.21.42)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (1.21.2)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: pyinstrument==3.4.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from smdebug) (3.4.2)\n",
      "Requirement already satisfied: pyinstrument-cext>=0.2.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pyinstrument==3.4.2->smdebug) (0.2.4)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /home/ec2-user/.local/lib/python3.8/site-packages (from ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipywidgets) (7.32.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipywidgets) (6.5.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipywidgets) (1.0.2)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.5.2)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.42 in /home/ec2-user/.local/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.24.46)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.6.0)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.1.0)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pickleshare in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: backcall in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (59.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.22)\n",
      "Requirement already satisfied: pygments in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.17.2)\n",
      "Requirement already satisfied: jupyter-core in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (4.9.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging>=20.0->sagemaker) (3.0.6)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->sagemaker) (2021.3)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from botocore<1.25.0,>=1.24.42->boto3<2.0,>=1.20.21->sagemaker) (1.26.8)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: entrypoints in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: pyzmq>=13 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.12.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.12.0)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.3)\n",
      "Requirement already satisfied: nbconvert in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.3.0)\n",
      "Requirement already satisfied: argon2-cffi in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.1.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: bleach in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: testpath in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.9)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (0.12.0)\n",
      "Requirement already satisfied: split-folders in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (0.5.1)\n",
      "Requirement already satisfied: wget in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (3.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchvision) (4.0.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchvision) (1.21.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchvision) (2.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: torch==1.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->torchvision) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->torchvision) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->torchvision) (2.0.7)\n",
      "Add data-root and default-shm-size=10G\n",
      "Redirecting to /bin/systemctl restart docker.service\n",
      "Docker Restart\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install --upgrade pip \n",
    "    !{sys.executable} -m pip install -U sagemaker smdebug ipywidgets --user \n",
    "    !{sys.executable} -m pip install -U torchvision split-folders wget\n",
    "    !/bin/bash ./local/local_change_setting.sh\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.debugger import (\n",
    "    Rule, ProfilerRule, rule_configs, ProfilerConfig, \n",
    "    FrameworkProfile, DetailedProfilingConfig, \n",
    "    DataloaderProfilingConfig, PythonProfilingConfig)\n",
    "import time\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.94.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Getting the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the data to S3\n",
    "We are going to use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use later when we start the training job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wget\n",
    "# import tarfile\n",
    "# import shutil\n",
    "# import os\n",
    "# import glob\n",
    "# import splitfolders\n",
    "\n",
    "# def make_dir(img_path, delete=True):\n",
    "#     import shutil, os\n",
    "#     try:\n",
    "#         if not os.path.exists(img_path):\n",
    "#             os.makedirs(img_path)\n",
    "#         else:\n",
    "#             if delete:\n",
    "#                 shutil.rmtree(img_path)\n",
    "#     except OSError:\n",
    "#         print(\"Error\")\n",
    "\n",
    "# rawimg_path = 'raw_img'\n",
    "# output_path = 'data'\n",
    "# dataset_path = './dataset'\n",
    "\n",
    "# make_dir(rawimg_path)\n",
    "# make_dir(dataset_path)\n",
    "\n",
    "# if not (os.path.isfile(\"images.tar.gz\") and tarfile.is_tarfile(\"images.tar.gz\")):\n",
    "#     wget.download('https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz')\n",
    "# tar = tarfile.open(\"images.tar.gz\")\n",
    "# tar.extractall(path=rawimg_path)\n",
    "# tar.close()\n",
    "\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# def checkImage(path):\n",
    "# #     print(path)\n",
    "#     try:\n",
    "#         with open(path, 'rb') as f:\n",
    "#             data = f.read()\n",
    "#             f.seek(-2,2)\n",
    "#             value = f.read()\n",
    "\n",
    "#         encoded_img = np.frombuffer(data, dtype = np.uint8)\n",
    "#         img_cv = cv2.imdecode(encoded_img, cv2.IMREAD_COLOR)\n",
    "# #         print(img_cv)\n",
    "#         if img_cv.shape[0]>0 and value == b'\\xff\\xd9':\n",
    "#             return True\n",
    "#     except:\n",
    "#         return False\n",
    "\n",
    "# corrupt_img = ['Egyptian_Mau_14.jpg','Egyptian_Mau_139.jpg','Egyptian_Mau_145.jpg','Egyptian_Mau_156.jpg',\n",
    "#                'Egyptian_Mau_167.jpg','Egyptian_Mau_177.jpg','Egyptian_Mau_186.jpg','Egyptian_Mau_191.jpg',\n",
    "#                'Abyssinian_5.jpg','Abyssinian_34.jpg','chihuahua_121.jpg','beagle_116.jpg']\n",
    "\n",
    "# file_dir = os.path.join(rawimg_path, 'images')\n",
    "# output_dir = os.path.join(rawimg_path, output_path)\n",
    "\n",
    "# for file_path in glob.glob(file_dir + \"/*\"):\n",
    "#     filename = file_path.split(\"/\")[2]\n",
    "#     if checkImage(file_path) and filename not in corrupt_img:\n",
    "#         dir_name = filename.split(\"_\")\n",
    "#         dir_name.pop()\n",
    "#         dir_name = \"_\".join(dir_name)\n",
    "#         dir_path = os.path.join(output_dir, dir_name)\n",
    "#         make_dir(dir_path, False)\n",
    "#         target_path = os.path.join(dir_path, filename)\n",
    "#         shutil.copyfile(file_path, target_path)\n",
    "#     else:\n",
    "#         print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num = 0\n",
    "# for file_path in glob.glob(output_dir + \"/*/*\"):\n",
    "# #     print(file_path)\n",
    "#     if not checkImage(file_path):\n",
    "# #         print(file_path)\n",
    "#         num += 1\n",
    "# print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitfolders.ratio(output_dir, output=dataset_path, seed=1337, ratio=(.8, .1, .1)) # default values\n",
    "# inputs = 's3://{}/{}'.format(bucket, 'oxford_pet_dataset')\n",
    "# !aws s3 rm $inputs --quiet --recursive\n",
    "# !aws s3 cp ./dataset $inputs --quiet --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 's3://sagemaker-us-west-2-322537213286/oxford_pet_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Training script\n",
    "The script provides all the code we need for training and hosting a SageMaker model (`model_fn` function to load a model).\n",
    "The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to.\n",
    "  These artifacts are uploaded to S3 for model hosting.\n",
    "\n",
    "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's `fit()` method, the following will be set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAINING`: A string representing the path to the directory containing data in the 'training' channel.\n",
    "\n",
    "For more information about training environment variables, please visit [SageMaker Containers](https://github.com/aws/sagemaker-containers).\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance.\n",
    "\n",
    "Because the SageMaker imports the training script, you should put your training code in a main guard (``if __name__=='__main__':``) if you are using the same script to host your model as we do in this example, so that SageMaker does not inadvertently run your training code at the wrong point in execution.\n",
    "\n",
    "For example, the script run by this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pygmentize ./train_code/pytorch_mnist_smdp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'epochs': 1,\n",
    "    'batch_size':1,\n",
    "    'img_size':6000, #5000(p3, p4d 성공), #6000, #10000,\n",
    "    'ddp' : True,\n",
    "    'smp' : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smdistributed': {'modelparallel': {'enabled': True,\n",
       "   'parameters': {'ddp': True,\n",
       "    'partitions': 4,\n",
       "    'prescaled_batch': False,\n",
       "    'fp16_params': True,\n",
       "    'optimize': 'memory',\n",
       "    'auto_partition': True,\n",
       "    'microbatches': 1}}},\n",
       " 'mpi': {'enabled': True,\n",
       "  'processes_per_host': 8,\n",
       "  'custom_mpi_options': '-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1'}}"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution={}\n",
    "\n",
    "if hyperparameters.get('smp'):\n",
    "    distribution_hyper = {\n",
    "        \"ddp\": hyperparameters['ddp'],\n",
    "#         \"tensor_parallel_degree\": 4,\n",
    "        \"partitions\": 4,  ## pipeline_parallel_degree\n",
    "    #     \"shard_optimizer_state\": True,\n",
    "        \"prescaled_batch\": False,\n",
    "        \"fp16_params\": True,\n",
    "        \"optimize\": \"memory\", #\"speed\",\n",
    "        \"auto_partition\": True,\n",
    "    #     \"default_partition\": 0,\n",
    "        \"microbatches\":1\n",
    "    }\n",
    "    \n",
    "    hyperparameters['prescaled_batch'] = distribution_hyper['prescaled_batch']\n",
    "    \n",
    "    distribution['smdistributed'] = {\n",
    "        \"modelparallel\": {\n",
    "            \"enabled\":True,\n",
    "            \"parameters\": distribution_hyper\n",
    "        }   \n",
    "    }\n",
    "    mpioptions = \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR \"\n",
    "    mpioptions += \"-x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 \"\n",
    "    mpioptions += \"-x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\"\n",
    "\n",
    "\n",
    "    distribution[\"mpi\"]={\n",
    "        \"enabled\": True,\n",
    "        \"processes_per_host\": 8, # Pick your processes_per_host\n",
    "        \"custom_mpi_options\": mpioptions      \n",
    "    }\n",
    "else:\n",
    "    distribution = {\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution = {\"smdistributed\": {\n",
    "#                     \"dataparallel\": {\n",
    "#                             \"enabled\": True\n",
    "#                     }\n",
    "#                }\n",
    "#              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution = {\"mpi\": {\n",
    "#                     \"enabled\": True\n",
    "#                }\n",
    "#              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = 'ml.p4d.24xlarge'\n",
    "instance_type = 'local_gpu'\n",
    "instance_count = 1\n",
    "entry_point = 'main.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo rm -rf /home/ec2-user/SageMaker/temp\n",
    "!mkdir /home/ec2-user/SageMaker/temp\n",
    "!sudo rm -rf /tmp/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "if instance_type =='local_gpu':\n",
    "    from sagemaker.local import LocalSession\n",
    "    from pathlib import Path\n",
    "\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}, 'container_root':'/home/ec2-user/SageMaker/temp'}\n",
    "    s3_data_path = f'file://{Path.cwd()}/dataset'\n",
    "    source_dir = f'{Path.cwd()}/code'\n",
    "    checkpoint_s3_bucket = None\n",
    "else:\n",
    "    sess = boto3.Session()\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    sm = sess.client('sagemaker')\n",
    "    s3_data_path = inputs\n",
    "    source_dir = 'code'\n",
    "    checkpoint_s3_bucket = f's3://{bucket}/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=entry_point,\n",
    "                    source_dir=source_dir,\n",
    "                    role=role,\n",
    "                    framework_version='1.10',\n",
    "                    py_version='py38',\n",
    "                    instance_count=instance_count,\n",
    "                    instance_type=instance_type,\n",
    "                    distribution=distribution,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    disable_profiler=True,\n",
    "                    debugger_hook_config=False,\n",
    "                    max_run=1*30*60,\n",
    "                    hyperparameters=hyperparameters\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo rm -rf code/core.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ppsbyzj1gc-algo-1-1c6hk ... \n",
      "Creating ppsbyzj1gc-algo-1-1c6hk ... done\n",
      "Attaching to ppsbyzj1gc-algo-1-1c6hk\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m 2022-06-14 00:24:34,240 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m 2022-06-14 00:24:34,325 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m 2022-06-14 00:24:34,328 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m 2022-06-14 00:24:34,807 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m 2022-06-14 00:24:34,807 sagemaker-training-toolkit INFO     Creating SSH daemon.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m 2022-06-14 00:24:34,809 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m 2022-06-14 00:24:34,809 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1-1c6hk'] Hosts: ['algo-1-1c6hk:8'] process_per_hosts: 8 num_processes: 8\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m 2022-06-14 00:24:34,810 sagemaker-training-toolkit INFO     Network interface name: eth0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m 2022-06-14 00:24:34,885 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m Training Env:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m {\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"additional_framework_parameters\": {\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"sagemaker_mpi_enabled\": true,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"sagemaker_mpi_num_of_processes_per_host\": 8,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"sagemaker_mpi_custom_mpi_options\": \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"sagemaker_instance_type\": \"local_gpu\"\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     },\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"training\": \"/opt/ml/input/data/training\"\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     },\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"current_host\": \"algo-1-1c6hk\",\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"algo-1-1c6hk\"\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     ],\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"epochs\": 1,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"batch_size\": 1,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"img_size\": 6000,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"ddp\": true,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"smp\": true,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"prescaled_batch\": false,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"mp_parameters\": {\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m             \"ddp\": true,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m             \"partitions\": 4,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m             \"prescaled_batch\": false,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m             \"fp16_params\": true,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m             \"optimize\": \"memory\",\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m             \"auto_partition\": true,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m             \"microbatches\": 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         }\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     },\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"training\": {\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         }\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     },\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"job_name\": \"training-job-1655166271\",\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"master_hostname\": \"algo-1-1c6hk\",\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"module_dir\": \"/opt/ml/code\",\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"module_name\": \"main\",\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"num_cpus\": 64,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"num_gpus\": 8,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"current_host\": \"algo-1-1c6hk\",\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m             \"algo-1-1c6hk\"\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m         ]\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     },\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m     \"user_entry_point\": \"main.py\"\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m }\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m Environment variables:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_HOSTS=[\"algo-1-1c6hk\"]\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_HPS={\"batch_size\":1,\"ddp\":true,\"epochs\":1,\"img_size\":6000,\"mp_parameters\":{\"auto_partition\":true,\"ddp\":true,\"fp16_params\":true,\"microbatches\":1,\"optimize\":\"memory\",\"partitions\":4,\"prescaled_batch\":false},\"prescaled_batch\":false,\"smp\":true}\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_USER_ENTRY_POINT=main.py\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"local_gpu\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-1c6hk\",\"hosts\":[\"algo-1-1c6hk\"]}\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_INPUT_DATA_CONFIG={\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_CHANNELS=[\"training\"]\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_CURRENT_HOST=algo-1-1c6hk\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_MODULE_NAME=main\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_NUM_CPUS=64\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_NUM_GPUS=8\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_MODULE_DIR=/opt/ml/code\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"local_gpu\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-1c6hk\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-1c6hk\"],\"hyperparameters\":{\"batch_size\":1,\"ddp\":true,\"epochs\":1,\"img_size\":6000,\"mp_parameters\":{\"auto_partition\":true,\"ddp\":true,\"fp16_params\":true,\"microbatches\":1,\"optimize\":\"memory\",\"partitions\":4,\"prescaled_batch\":false},\"prescaled_batch\":false,\"smp\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"training-job-1655166271\",\"log_level\":20,\"master_hostname\":\"algo-1-1c6hk\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"main\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-1c6hk\",\"hosts\":[\"algo-1-1c6hk\"]},\"user_entry_point\":\"main.py\"}\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_USER_ARGS=[\"--batch_size\",\"1\",\"--ddp\",\"True\",\"--epochs\",\"1\",\"--img_size\",\"6000\",\"--mp_parameters\",\"auto_partition=True,ddp=True,fp16_params=True,microbatches=1,optimize=memory,partitions=4,prescaled_batch=False\",\"--prescaled_batch\",\"False\",\"--smp\",\"True\"]\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_HP_BATCH_SIZE=1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_HP_IMG_SIZE=6000\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_HP_DDP=true\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_HP_SMP=true\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_HP_PRESCALED_BATCH=false\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m SM_HP_MP_PARAMETERS={\"auto_partition\":true,\"ddp\":true,\"fp16_params\":true,\"microbatches\":1,\"optimize\":\"memory\",\"partitions\":4,\"prescaled_batch\":false}\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220524-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m mpirun --host algo-1-1c6hk:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAINING -x SM_HP_EPOCHS -x SM_HP_BATCH_SIZE -x SM_HP_IMG_SIZE -x SM_HP_DDP -x SM_HP_SMP -x SM_HP_PRESCALED_BATCH -x SM_HP_MP_PARAMETERS -x PYTHONPATH /opt/conda/bin/python3.8 -m mpi4py main.py --batch_size 1 --ddp True --epochs 1 --img_size 6000 --mp_parameters auto_partition=True,ddp=True,fp16_params=True,microbatches=1,optimize=memory,partitions=4,prescaled_batch=False --prescaled_batch False --smp True\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m Data for JOB [46271,1] offset 0 Total slots allocated 8\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m  ========================   JOB MAP   ========================\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m  Data for node: algo-1-1c6hk\tNum slots: 8\tMax slots: 0\tNum procs: 8\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m  \tProcess OMPI jobid: [46271,1] App: 0 Process rank: 0 Bound: N/A\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m  \tProcess OMPI jobid: [46271,1] App: 0 Process rank: 1 Bound: N/A\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m  \tProcess OMPI jobid: [46271,1] App: 0 Process rank: 2 Bound: N/A\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m  \tProcess OMPI jobid: [46271,1] App: 0 Process rank: 3 Bound: N/A\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m  \tProcess OMPI jobid: [46271,1] App: 0 Process rank: 4 Bound: N/A\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m  \tProcess OMPI jobid: [46271,1] App: 0 Process rank: 5 Bound: N/A\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m  \tProcess OMPI jobid: [46271,1] App: 0 Process rank: 6 Bound: N/A\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m  \tProcess OMPI jobid: [46271,1] App: 0 Process rank: 7 Bound: N/A\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m  =============================================================\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 5\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 7\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 2\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 6\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 3\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stdout>:[2022-06-14 00:24:38.989: I smdistributed/modelparallel/torch/state_mod.py:162] [3] Finished initializing torch distributed process groups. pp_rank: 3, tp_rank: 0, dp_rank: 0, rdp_rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stdout>:[2022-06-14 00:24:38.989: I smdistributed/modelparallel/torch/state_mod.py:162] [4] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 1, rdp_rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stdout>:[2022-06-14 00:24:38.989: I smdistributed/modelparallel/torch/state_mod.py:162] [5] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 1, rdp_rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stdout>:[2022-06-14 00:24:38.993: I smdistributed/modelparallel/torch/state_mod.py:162] [2] Finished initializing torch distributed process groups. pp_rank: 2, tp_rank: 0, dp_rank: 0, rdp_rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stdout>:[2022-06-14 00:24:38.994: I smdistributed/modelparallel/torch/state_mod.py:162] [7] Finished initializing torch distributed process groups. pp_rank: 3, tp_rank: 0, dp_rank: 1, rdp_rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stdout>:[2022-06-14 00:24:38.994: I smdistributed/modelparallel/torch/state_mod.py:162] [6] Finished initializing torch distributed process groups. pp_rank: 2, tp_rank: 0, dp_rank: 1, rdp_rank: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:38.999: I smdistributed/modelparallel/torch/state_mod.py:162] [0] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 0, rdp_rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stdout>:[2022-06-14 00:24:38.999: I smdistributed/modelparallel/torch/state_mod.py:162] [1] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 0, rdp_rank: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.000: I smdistributed/modelparallel/torch/throttler.py:37] Using NCCL throttle limit of 1.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.266: I smdistributed/modelparallel/backend/config.py:234] Configuration parameters:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.267: I smdistributed/modelparallel/backend/config.py:237]   pipeline_parallel_degree: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.267: I smdistributed/modelparallel/backend/config.py:237]   microbatches: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.267: I smdistributed/modelparallel/backend/config.py:237]   pipeline: interleaved\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.267: I smdistributed/modelparallel/backend/config.py:237]   horovod: False\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.267: I smdistributed/modelparallel/backend/config.py:237]   ddp: True\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.267: I smdistributed/modelparallel/backend/config.py:237]   tensor_parallel_degree: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.267: I smdistributed/modelparallel/backend/config.py:237]   ddp_port: None\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.267: I smdistributed/modelparallel/backend/config.py:237]   ddp_dist_backend: nccl\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.268: I smdistributed/modelparallel/backend/config.py:237]   contiguous: True\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.268: I smdistributed/modelparallel/backend/config.py:237]   placement_strategy: cluster\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.268: I smdistributed/modelparallel/backend/config.py:237]   optimize: memory\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.268: I smdistributed/modelparallel/backend/config.py:237]   default_partition: None\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.268: I smdistributed/modelparallel/backend/config.py:237]   auto_partition: True\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.268: I smdistributed/modelparallel/backend/config.py:237]   prescaled_batch: False\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.268: I smdistributed/modelparallel/backend/config.py:237]   memory_weight: 0.8\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.268: I smdistributed/modelparallel/backend/config.py:237]   active_microbatches: 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.268: I smdistributed/modelparallel/backend/config.py:237]   fp16_params: True\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.269: I smdistributed/modelparallel/backend/config.py:237]   tensor_parallel_seed: 0\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.269: I smdistributed/modelparallel/backend/config.py:237]   offload_activations: False\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.269: I smdistributed/modelparallel/backend/config.py:237]   shard_optimizer_state: False\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.269: I smdistributed/modelparallel/backend/config.py:237]   skip_tracing: False\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:24:39.269: I smdistributed/modelparallel/backend/config.py:237]   activation_loading_horizon: 4\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:5,algo-1]<stdout>:=> using pre-trained model 'vgg11'\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:3,algo-1]<stdout>:=> using pre-trained model 'vgg11'\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:7,algo-1]<stdout>:=> using pre-trained model 'vgg11'\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:6,algo-1]<stdout>:=> using pre-trained model 'vgg11'\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:2,algo-1]<stdout>:=> using pre-trained model 'vgg11'\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:4,algo-1]<stdout>:=> using pre-trained model 'vgg11'\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:=> using pre-trained model 'vgg11'\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:1,algo-1]<stdout>:=> using pre-trained model 'vgg11'\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:Downloading: \"https://download.pytorch.org/models/vgg11-8a719046.pth\" to /root/.cache/torch/hub/checkpoints/vgg11-8a719046.pth\n",
      "  0%|          | 0.00/507M [00:00<?, ?B/s]ank:0,algo-1]<stderr>:\n",
      "  0%|          | 32.0k/507M [00:00<1:01:15, 145kB/s]-1]<stderr>:\n",
      "  0%|          | 96.0k/507M [00:00<35:38, 248kB/s]go-1]<stderr>:\n",
      "  0%|          | 176k/507M [00:00<21:39, 409kB/s]lgo-1]<stderr>:\n",
      "  0%|          | 232k/507M [00:00<20:11, 438kB/s]lgo-1]<stderr>:\n",
      "  0%|          | 400k/507M [00:00<11:04, 799kB/s]lgo-1]<stderr>:\n",
      "  0%|          | 496k/507M [00:00<10:38, 831kB/s]lgo-1]<stderr>:\n",
      "  0%|          | 832k/507M [00:00<05:36, 1.58MB/s]go-1]<stderr>:\n",
      "  0%|          | 0.98M/507M [00:01<05:26, 1.62MB/s]o-1]<stderr>:\n",
      "  0%|          | 1.62M/507M [00:01<02:52, 3.08MB/s]o-1]<stderr>:\n",
      "  0%|          | 1.94M/507M [00:01<03:03, 2.88MB/s]o-1]<stderr>:\n",
      "  0%|          | 2.43M/507M [00:01<02:31, 3.49MB/s]o-1]<stderr>:\n",
      "  1%|          | 2.79M/507M [00:01<02:28, 3.55MB/s]o-1]<stderr>:\n",
      "  1%|          | 3.30M/507M [00:01<02:10, 4.06MB/s]o-1]<stderr>:\n",
      "  1%|          | 3.70M/507M [00:01<02:11, 4.02MB/s]o-1]<stderr>:\n",
      "  1%|          | 4.24M/507M [00:01<01:57, 4.48MB/s]o-1]<stderr>:\n",
      "  1%|          | 4.68M/507M [00:01<01:59, 4.39MB/s]o-1]<stderr>:\n",
      "  1%|          | 5.24M/507M [00:02<01:49, 4.81MB/s]o-1]<stderr>:\n",
      "  1%|          | 5.71M/507M [00:02<01:52, 4.68MB/s]o-1]<stderr>:\n",
      "  1%|          | 6.31M/507M [00:02<01:42, 5.12MB/s]o-1]<stderr>:\n",
      "  1%|▏         | 6.81M/507M [00:02<01:45, 4.97MB/s]o-1]<stderr>:\n",
      "  1%|▏         | 7.44M/507M [00:02<01:37, 5.39MB/s]o-1]<stderr>:\n",
      "  2%|▏         | 7.96M/507M [00:02<01:39, 5.25MB/s]o-1]<stderr>:\n",
      "  2%|▏         | 8.61M/507M [00:02<01:31, 5.69MB/s]o-1]<stderr>:\n",
      "  2%|▏         | 9.16M/507M [00:02<01:34, 5.50MB/s]o-1]<stderr>:\n",
      "  2%|▏         | 9.84M/507M [00:02<01:27, 5.97MB/s]o-1]<stderr>:\n",
      "  2%|▏         | 10.4M/507M [00:02<01:30, 5.78MB/s]o-1]<stderr>:\n",
      "  2%|▏         | 11.2M/507M [00:03<01:22, 6.29MB/s]o-1]<stderr>:\n",
      "  2%|▏         | 11.8M/507M [00:03<01:25, 6.10MB/s]o-1]<stderr>:\n",
      "  2%|▏         | 12.5M/507M [00:03<01:18, 6.62MB/s]o-1]<stderr>:\n",
      "  3%|▎         | 13.2M/507M [00:03<01:20, 6.42MB/s]o-1]<stderr>:\n",
      "  3%|▎         | 14.0M/507M [00:03<01:14, 6.94MB/s]o-1]<stderr>:\n",
      "  3%|▎         | 14.6M/507M [00:03<01:16, 6.72MB/s]o-1]<stderr>:\n",
      "  3%|▎         | 15.5M/507M [00:03<01:10, 7.32MB/s]o-1]<stderr>:\n",
      "  3%|▎         | 16.2M/507M [00:03<01:13, 7.03MB/s]o-1]<stderr>:\n",
      "  3%|▎         | 17.0M/507M [00:03<01:08, 7.53MB/s]o-1]<stderr>:\n",
      "  4%|▎         | 17.8M/507M [00:04<01:11, 7.18MB/s]o-1]<stderr>:\n",
      "  4%|▎         | 18.6M/507M [00:04<01:07, 7.64MB/s]o-1]<stderr>:\n",
      "  4%|▍         | 19.3M/507M [00:04<01:10, 7.26MB/s]o-1]<stderr>:\n",
      "  4%|▍         | 20.2M/507M [00:04<01:06, 7.68MB/s]o-1]<stderr>:\n",
      "  4%|▍         | 20.9M/507M [00:04<01:09, 7.30MB/s]o-1]<stderr>:\n",
      "  4%|▍         | 21.8M/507M [00:04<01:05, 7.72MB/s]o-1]<stderr>:\n",
      "  4%|▍         | 22.5M/507M [00:04<01:09, 7.33MB/s]o-1]<stderr>:\n",
      "  5%|▍         | 23.4M/507M [00:04<01:10, 7.18MB/s]o-1]<stderr>:\n",
      "  5%|▍         | 24.2M/507M [00:04<01:07, 7.52MB/s]o-1]<stderr>:\n",
      "  5%|▍         | 24.9M/507M [00:05<01:09, 7.29MB/s]o-1]<stderr>:\n",
      "  5%|▌         | 25.7M/507M [00:05<01:06, 7.61MB/s]o-1]<stderr>:\n",
      "  5%|▌         | 26.4M/507M [00:05<01:08, 7.33MB/s]o-1]<stderr>:\n",
      "  5%|▌         | 27.3M/507M [00:05<01:05, 7.69MB/s]o-1]<stderr>:\n",
      "  6%|▌         | 28.0M/507M [00:05<01:08, 7.33MB/s]o-1]<stderr>:\n",
      "  6%|▌         | 28.9M/507M [00:05<01:04, 7.74MB/s]o-1]<stderr>:\n",
      "  6%|▌         | 29.6M/507M [00:05<01:07, 7.38MB/s]o-1]<stderr>:\n",
      "  6%|▌         | 30.4M/507M [00:05<01:04, 7.72MB/s]o-1]<stderr>:\n",
      "  6%|▌         | 31.2M/507M [00:05<01:07, 7.38MB/s]o-1]<stderr>:\n",
      "  6%|▋         | 32.0M/507M [00:06<01:04, 7.76MB/s]o-1]<stderr>:\n",
      "  6%|▋         | 32.8M/507M [00:06<01:07, 7.35MB/s]o-1]<stderr>:\n",
      "  7%|▋         | 33.6M/507M [00:06<01:03, 7.79MB/s]o-1]<stderr>:\n",
      "  7%|▋         | 34.4M/507M [00:06<01:06, 7.40MB/s]o-1]<stderr>:\n",
      "  7%|▋         | 35.2M/507M [00:06<01:03, 7.80MB/s]o-1]<stderr>:\n",
      "  7%|▋         | 36.0M/507M [00:06<01:06, 7.39MB/s]o-1]<stderr>:\n",
      "  7%|▋         | 36.9M/507M [00:06<01:03, 7.78MB/s]o-1]<stderr>:\n",
      "  7%|▋         | 37.6M/507M [00:06<01:06, 7.43MB/s]o-1]<stderr>:\n",
      "  8%|▊         | 38.5M/507M [00:06<01:02, 7.80MB/s]o-1]<stderr>:\n",
      "  8%|▊         | 39.2M/507M [00:07<01:06, 7.42MB/s]o-1]<stderr>:\n",
      "  8%|▊         | 40.0M/507M [00:07<01:07, 7.24MB/s]o-1]<stderr>:\n",
      "  8%|▊         | 40.8M/507M [00:07<01:05, 7.49MB/s]o-1]<stderr>:\n",
      "  8%|▊         | 41.5M/507M [00:07<01:06, 7.34MB/s]o-1]<stderr>:\n",
      "  8%|▊         | 42.3M/507M [00:07<01:04, 7.58MB/s]o-1]<stderr>:\n",
      "  8%|▊         | 43.0M/507M [00:07<01:06, 7.33MB/s]o-1]<stderr>:\n",
      "  9%|▊         | 43.9M/507M [00:07<01:03, 7.66MB/s]o-1]<stderr>:\n",
      "  9%|▉         | 44.6M/507M [00:07<01:05, 7.41MB/s]o-1]<stderr>:\n",
      "  9%|▉         | 45.4M/507M [00:07<01:03, 7.67MB/s]o-1]<stderr>:\n",
      "  9%|▉         | 46.2M/507M [00:08<01:05, 7.40MB/s]o-1]<stderr>:\n",
      "  9%|▉         | 47.0M/507M [00:08<01:02, 7.71MB/s]o-1]<stderr>:\n",
      "  9%|▉         | 47.7M/507M [00:08<01:04, 7.41MB/s]o-1]<stderr>:\n",
      " 10%|▉         | 48.5M/507M [00:08<01:02, 7.71MB/s]o-1]<stderr>:\n",
      " 10%|▉         | 49.3M/507M [00:08<01:04, 7.42MB/s]o-1]<stderr>:\n",
      " 10%|▉         | 50.1M/507M [00:08<01:01, 7.76MB/s]o-1]<stderr>:\n",
      " 10%|█         | 50.9M/507M [00:08<01:04, 7.40MB/s]o-1]<stderr>:\n",
      " 10%|█         | 51.7M/507M [00:08<01:01, 7.78MB/s]o-1]<stderr>:\n",
      " 10%|█         | 52.5M/507M [00:08<01:04, 7.41MB/s]o-1]<stderr>:\n",
      " 11%|█         | 53.3M/507M [00:08<01:00, 7.80MB/s]o-1]<stderr>:\n",
      " 11%|█         | 54.1M/507M [00:09<01:03, 7.45MB/s]o-1]<stderr>:\n",
      " 11%|█         | 54.9M/507M [00:09<01:00, 7.82MB/s]o-1]<stderr>:\n",
      " 11%|█         | 55.7M/507M [00:09<01:03, 7.46MB/s]o-1]<stderr>:\n",
      " 11%|█         | 56.5M/507M [00:09<01:00, 7.84MB/s]o-1]<stderr>:\n",
      " 11%|█▏        | 57.3M/507M [00:09<01:03, 7.43MB/s]o-1]<stderr>:\n",
      " 11%|█▏        | 58.1M/507M [00:09<01:00, 7.83MB/s]o-1]<stderr>:\n",
      " 12%|█▏        | 58.9M/507M [00:09<01:03, 7.44MB/s]o-1]<stderr>:\n",
      " 12%|█▏        | 59.7M/507M [00:09<00:59, 7.85MB/s]o-1]<stderr>:\n",
      " 12%|█▏        | 60.5M/507M [00:09<01:02, 7.45MB/s]o-1]<stderr>:\n",
      " 12%|█▏        | 61.3M/507M [00:10<01:03, 7.32MB/s]o-1]<stderr>:\n",
      " 12%|█▏        | 62.1M/507M [00:10<01:01, 7.60MB/s]o-1]<stderr>:\n",
      " 12%|█▏        | 62.8M/507M [00:10<01:02, 7.47MB/s]o-1]<stderr>:\n",
      " 13%|█▎        | 63.6M/507M [00:10<01:00, 7.63MB/s]o-1]<stderr>:\n",
      " 13%|█▎        | 64.3M/507M [00:10<01:01, 7.49MB/s]o-1]<stderr>:\n",
      " 13%|█▎        | 65.1M/507M [00:10<01:00, 7.67MB/s]o-1]<stderr>:\n",
      " 13%|█▎        | 65.9M/507M [00:10<01:01, 7.49MB/s]o-1]<stderr>:\n",
      " 13%|█▎        | 66.7M/507M [00:10<00:59, 7.69MB/s]o-1]<stderr>:\n",
      " 13%|█▎        | 67.4M/507M [00:10<01:01, 7.48MB/s]o-1]<stderr>:\n",
      " 13%|█▎        | 68.2M/507M [00:11<00:59, 7.72MB/s]o-1]<stderr>:\n",
      " 14%|█▎        | 68.9M/507M [00:11<01:01, 7.51MB/s]o-1]<stderr>:\n",
      " 14%|█▍        | 69.7M/507M [00:11<00:59, 7.74MB/s]o-1]<stderr>:\n",
      " 14%|█▍        | 70.5M/507M [00:11<01:01, 7.48MB/s]o-1]<stderr>:\n",
      " 14%|█▍        | 71.3M/507M [00:11<00:59, 7.72MB/s]o-1]<stderr>:\n",
      " 14%|█▍        | 72.0M/507M [00:11<01:00, 7.48MB/s]o-1]<stderr>:\n",
      " 14%|█▍        | 72.8M/507M [00:11<00:59, 7.70MB/s]o-1]<stderr>:\n",
      " 15%|█▍        | 73.6M/507M [00:11<01:00, 7.48MB/s]o-1]<stderr>:\n",
      " 15%|█▍        | 74.4M/507M [00:11<00:58, 7.76MB/s]o-1]<stderr>:\n",
      " 15%|█▍        | 75.1M/507M [00:12<01:00, 7.54MB/s]o-1]<stderr>:\n",
      " 15%|█▍        | 75.9M/507M [00:12<00:58, 7.77MB/s]o-1]<stderr>:\n",
      " 15%|█▌        | 76.7M/507M [00:12<00:59, 7.53MB/s]o-1]<stderr>:\n",
      " 15%|█▌        | 77.5M/507M [00:12<00:57, 7.81MB/s]o-1]<stderr>:\n",
      " 15%|█▌        | 78.2M/507M [00:12<00:59, 7.60MB/s]o-1]<stderr>:\n",
      " 16%|█▌        | 79.0M/507M [00:12<00:57, 7.82MB/s]o-1]<stderr>:\n",
      " 16%|█▌        | 79.8M/507M [00:12<00:58, 7.60MB/s]o-1]<stderr>:\n",
      " 16%|█▌        | 80.6M/507M [00:12<00:56, 7.85MB/s]o-1]<stderr>:\n",
      " 16%|█▌        | 81.4M/507M [00:12<00:58, 7.65MB/s]o-1]<stderr>:\n",
      " 16%|█▌        | 82.2M/507M [00:12<00:56, 7.85MB/s]o-1]<stderr>:\n",
      " 16%|█▋        | 82.9M/507M [00:13<00:58, 7.64MB/s]o-1]<stderr>:\n",
      " 17%|█▋        | 83.8M/507M [00:13<00:56, 7.88MB/s]o-1]<stderr>:\n",
      " 17%|█▋        | 84.5M/507M [00:13<00:57, 7.64MB/s]o-1]<stderr>:\n",
      " 17%|█▋        | 85.3M/507M [00:13<00:56, 7.87MB/s]o-1]<stderr>:\n",
      " 17%|█▋        | 86.1M/507M [00:13<00:57, 7.62MB/s]o-1]<stderr>:\n",
      " 17%|█▋        | 86.9M/507M [00:13<00:55, 7.91MB/s]o-1]<stderr>:\n",
      " 17%|█▋        | 87.7M/507M [00:13<00:57, 7.66MB/s]o-1]<stderr>:\n",
      " 17%|█▋        | 88.5M/507M [00:13<00:55, 7.93MB/s]o-1]<stderr>:\n",
      " 18%|█▊        | 89.3M/507M [00:13<00:57, 7.66MB/s]o-1]<stderr>:\n",
      " 18%|█▊        | 90.1M/507M [00:14<00:55, 7.91MB/s]o-1]<stderr>:\n",
      " 18%|█▊        | 90.8M/507M [00:14<00:56, 7.66MB/s]o-1]<stderr>:\n",
      " 18%|█▊        | 91.7M/507M [00:14<00:55, 7.90MB/s]o-1]<stderr>:\n",
      " 18%|█▊        | 92.4M/507M [00:14<00:56, 7.66MB/s]o-1]<stderr>:\n",
      " 18%|█▊        | 93.2M/507M [00:14<00:54, 7.89MB/s]o-1]<stderr>:\n",
      " 19%|█▊        | 94.0M/507M [00:14<00:56, 7.66MB/s]o-1]<stderr>:\n",
      " 19%|█▊        | 94.8M/507M [00:14<00:54, 7.88MB/s]o-1]<stderr>:\n",
      " 19%|█▉        | 95.6M/507M [00:14<01:15, 5.68MB/s]o-1]<stderr>:\n",
      " 19%|█▉        | 96.5M/507M [00:15<01:20, 5.37MB/s]o-1]<stderr>:\n",
      " 19%|█▉        | 97.2M/507M [00:15<01:16, 5.63MB/s]o-1]<stderr>:\n",
      " 19%|█▉        | 97.8M/507M [00:15<01:19, 5.40MB/s]o-1]<stderr>:\n",
      " 19%|█▉        | 98.3M/507M [00:15<01:47, 3.98MB/s]o-1]<stderr>:\n",
      " 20%|█▉        | 99.6M/507M [00:15<01:27, 4.86MB/s]o-1]<stderr>:\n",
      " 20%|█▉        | 100M/507M [00:15<01:27, 4.88MB/s]go-1]<stderr>:\n",
      " 20%|█▉        | 101M/507M [00:16<01:33, 4.54MB/s]go-1]<stderr>:\n",
      " 20%|█▉        | 101M/507M [00:16<01:32, 4.62MB/s]go-1]<stderr>:\n",
      " 20%|██        | 102M/507M [00:16<01:37, 4.36MB/s]go-1]<stderr>:\n",
      " 20%|██        | 102M/507M [00:16<01:33, 4.52MB/s]go-1]<stderr>:\n",
      " 20%|██        | 103M/507M [00:16<01:38, 4.29MB/s]go-1]<stderr>:\n",
      " 20%|██        | 103M/507M [00:16<01:34, 4.47MB/s]go-1]<stderr>:\n",
      " 20%|██        | 103M/507M [00:16<01:39, 4.26MB/s]go-1]<stderr>:\n",
      " 20%|██        | 104M/507M [00:16<01:43, 4.10MB/s]go-1]<stderr>:\n",
      " 21%|██        | 104M/507M [00:16<01:36, 4.38MB/s]go-1]<stderr>:\n",
      " 21%|██        | 105M/507M [00:17<01:39, 4.22MB/s]go-1]<stderr>:\n",
      " 21%|██        | 105M/507M [00:17<01:34, 4.48MB/s]go-1]<stderr>:\n",
      " 21%|██        | 106M/507M [00:17<01:37, 4.31MB/s]go-1]<stderr>:\n",
      " 21%|██        | 106M/507M [00:17<01:32, 4.56MB/s]go-1]<stderr>:\n",
      " 21%|██        | 107M/507M [00:17<01:35, 4.37MB/s]go-1]<stderr>:\n",
      " 21%|██        | 107M/507M [00:17<01:30, 4.64MB/s]go-1]<stderr>:\n",
      " 21%|██        | 108M/507M [00:17<01:34, 4.44MB/s]go-1]<stderr>:\n",
      " 21%|██▏       | 108M/507M [00:17<01:29, 4.69MB/s]go-1]<stderr>:\n",
      " 21%|██▏       | 109M/507M [00:17<01:33, 4.48MB/s]go-1]<stderr>:\n",
      " 22%|██▏       | 109M/507M [00:18<01:28, 4.73MB/s]go-1]<stderr>:\n",
      " 22%|██▏       | 110M/507M [00:18<01:32, 4.51MB/s]go-1]<stderr>:\n",
      " 22%|██▏       | 110M/507M [00:18<01:26, 4.80MB/s]go-1]<stderr>:\n",
      " 22%|██▏       | 111M/507M [00:18<01:31, 4.55MB/s]go-1]<stderr>:\n",
      " 22%|██▏       | 111M/507M [00:18<01:25, 4.85MB/s]go-1]<stderr>:\n",
      " 22%|██▏       | 112M/507M [00:18<01:29, 4.62MB/s]go-1]<stderr>:\n",
      " 22%|██▏       | 112M/507M [00:18<01:24, 4.89MB/s]go-1]<stderr>:\n",
      " 22%|██▏       | 113M/507M [00:18<01:28, 4.65MB/s]go-1]<stderr>:\n",
      " 22%|██▏       | 113M/507M [00:18<01:31, 4.50MB/s]go-1]<stderr>:\n",
      " 22%|██▏       | 114M/507M [00:19<01:25, 4.81MB/s]go-1]<stderr>:\n",
      " 23%|██▎       | 114M/507M [00:19<01:29, 4.61MB/s]go-1]<stderr>:\n",
      " 23%|██▎       | 115M/507M [00:19<01:24, 4.89MB/s]go-1]<stderr>:\n",
      " 23%|██▎       | 115M/507M [00:19<01:27, 4.71MB/s]go-1]<stderr>:\n",
      " 23%|██▎       | 116M/507M [00:19<01:23, 4.94MB/s]go-1]<stderr>:\n",
      " 23%|██▎       | 116M/507M [00:19<01:26, 4.73MB/s]go-1]<stderr>:\n",
      " 23%|██▎       | 117M/507M [00:19<01:21, 4.99MB/s]go-1]<stderr>:\n",
      " 23%|██▎       | 117M/507M [00:19<01:25, 4.75MB/s]go-1]<stderr>:\n",
      " 23%|██▎       | 118M/507M [00:19<01:21, 5.01MB/s]go-1]<stderr>:\n",
      " 23%|██▎       | 118M/507M [00:20<01:25, 4.78MB/s]go-1]<stderr>:\n",
      " 23%|██▎       | 119M/507M [00:20<01:20, 5.03MB/s]go-1]<stderr>:\n",
      " 24%|██▎       | 119M/507M [00:20<01:25, 4.78MB/s]go-1]<stderr>:\n",
      " 24%|██▎       | 120M/507M [00:20<01:20, 5.05MB/s]go-1]<stderr>:\n",
      " 24%|██▎       | 120M/507M [00:20<01:24, 4.81MB/s]go-1]<stderr>:\n",
      " 24%|██▍       | 121M/507M [00:20<01:20, 5.04MB/s]go-1]<stderr>:\n",
      " 24%|██▍       | 121M/507M [00:20<01:23, 4.82MB/s]go-1]<stderr>:\n",
      " 24%|██▍       | 122M/507M [00:20<01:19, 5.07MB/s]go-1]<stderr>:\n",
      " 24%|██▍       | 122M/507M [00:20<01:23, 4.81MB/s]go-1]<stderr>:\n",
      " 24%|██▍       | 123M/507M [00:21<01:26, 4.67MB/s]go-1]<stderr>:\n",
      " 24%|██▍       | 124M/507M [00:21<01:20, 5.00MB/s]go-1]<stderr>:\n",
      " 24%|██▍       | 124M/507M [00:21<01:24, 4.77MB/s]go-1]<stderr>:\n",
      " 25%|██▍       | 125M/507M [00:21<01:19, 5.03MB/s]go-1]<stderr>:\n",
      " 25%|██▍       | 125M/507M [00:21<01:23, 4.79MB/s]go-1]<stderr>:\n",
      " 25%|██▍       | 126M/507M [00:21<01:18, 5.06MB/s]go-1]<stderr>:\n",
      " 25%|██▍       | 126M/507M [00:21<01:22, 4.84MB/s]go-1]<stderr>:\n",
      " 25%|██▍       | 127M/507M [00:21<01:18, 5.09MB/s]go-1]<stderr>:\n",
      " 25%|██▌       | 127M/507M [00:21<01:22, 4.83MB/s]go-1]<stderr>:\n",
      " 25%|██▌       | 128M/507M [00:22<01:18, 5.09MB/s]go-1]<stderr>:\n",
      " 25%|██▌       | 128M/507M [00:22<01:22, 4.83MB/s]go-1]<stderr>:\n",
      " 25%|██▌       | 129M/507M [00:22<01:17, 5.11MB/s]go-1]<stderr>:\n",
      " 25%|██▌       | 129M/507M [00:22<01:21, 4.84MB/s]go-1]<stderr>:\n",
      " 26%|██▌       | 130M/507M [00:22<01:17, 5.11MB/s]go-1]<stderr>:\n",
      " 26%|██▌       | 130M/507M [00:22<01:21, 4.84MB/s]go-1]<stderr>:\n",
      " 26%|██▌       | 131M/507M [00:22<01:23, 4.71MB/s]go-1]<stderr>:\n",
      " 26%|██▌       | 131M/507M [00:22<01:18, 4.99MB/s]go-1]<stderr>:\n",
      " 26%|██▌       | 132M/507M [00:22<01:21, 4.80MB/s]go-1]<stderr>:\n",
      " 26%|██▌       | 132M/507M [00:23<01:18, 5.03MB/s]go-1]<stderr>:\n",
      " 26%|██▌       | 133M/507M [00:23<01:21, 4.79MB/s]go-1]<stderr>:\n",
      " 26%|██▋       | 134M/507M [00:23<01:17, 5.07MB/s]go-1]<stderr>:\n",
      " 26%|██▋       | 134M/507M [00:23<01:20, 4.84MB/s]go-1]<stderr>:\n",
      " 27%|██▋       | 135M/507M [00:23<01:16, 5.07MB/s]go-1]<stderr>:\n",
      " 27%|██▋       | 135M/507M [00:23<01:20, 4.84MB/s]go-1]<stderr>:\n",
      " 27%|██▋       | 136M/507M [00:23<01:16, 5.09MB/s]go-1]<stderr>:\n",
      " 27%|██▋       | 136M/507M [00:23<01:20, 4.83MB/s]go-1]<stderr>:\n",
      " 27%|██▋       | 137M/507M [00:23<01:15, 5.11MB/s]go-1]<stderr>:\n",
      " 27%|██▋       | 137M/507M [00:24<01:19, 4.85MB/s]go-1]<stderr>:\n",
      " 27%|██▋       | 138M/507M [00:24<01:15, 5.13MB/s]go-1]<stderr>:\n",
      " 27%|██▋       | 138M/507M [00:24<01:19, 4.86MB/s]go-1]<stderr>:\n",
      " 27%|██▋       | 139M/507M [00:24<01:15, 5.14MB/s]go-1]<stderr>:\n",
      " 27%|██▋       | 139M/507M [00:24<01:19, 4.87MB/s]go-1]<stderr>:\n",
      " 28%|██▊       | 140M/507M [00:24<01:21, 4.71MB/s]go-1]<stderr>:\n",
      " 28%|██▊       | 140M/507M [00:24<01:16, 4.99MB/s]go-1]<stderr>:\n",
      " 28%|██▊       | 141M/507M [00:24<01:20, 4.79MB/s]go-1]<stderr>:\n",
      " 28%|██▊       | 141M/507M [00:24<01:16, 5.03MB/s]go-1]<stderr>:\n",
      " 28%|██▊       | 142M/507M [00:25<01:18, 4.85MB/s]go-1]<stderr>:\n",
      " 28%|██▊       | 142M/507M [00:25<01:15, 5.06MB/s]go-1]<stderr>:\n",
      " 28%|██▊       | 143M/507M [00:25<01:18, 4.86MB/s]go-1]<stderr>:\n",
      " 28%|██▊       | 143M/507M [00:25<01:14, 5.12MB/s]go-1]<stderr>:\n",
      " 28%|██▊       | 144M/507M [00:25<01:17, 4.89MB/s]go-1]<stderr>:\n",
      " 29%|██▊       | 145M/507M [00:25<01:14, 5.12MB/s]go-1]<stderr>:\n",
      " 29%|██▊       | 145M/507M [00:25<01:17, 4.92MB/s]go-1]<stderr>:\n",
      " 29%|██▊       | 146M/507M [00:25<01:13, 5.13MB/s]go-1]<stderr>:\n",
      " 29%|██▉       | 146M/507M [00:25<01:17, 4.89MB/s]go-1]<stderr>:\n",
      " 29%|██▉       | 147M/507M [00:26<01:12, 5.18MB/s]go-1]<stderr>:\n",
      " 29%|██▉       | 147M/507M [00:26<01:16, 4.91MB/s]go-1]<stderr>:\n",
      " 29%|██▉       | 148M/507M [00:26<01:12, 5.19MB/s]go-1]<stderr>:\n",
      " 29%|██▉       | 148M/507M [00:26<01:15, 4.95MB/s]go-1]<stderr>:\n",
      " 29%|██▉       | 149M/507M [00:26<01:12, 5.21MB/s]go-1]<stderr>:\n",
      " 29%|██▉       | 149M/507M [00:26<01:15, 4.97MB/s]go-1]<stderr>:\n",
      " 30%|██▉       | 150M/507M [00:26<01:16, 4.87MB/s]go-1]<stderr>:\n",
      " 30%|██▉       | 150M/507M [00:26<01:13, 5.12MB/s]go-1]<stderr>:\n",
      " 30%|██▉       | 151M/507M [00:26<01:15, 4.94MB/s]go-1]<stderr>:\n",
      " 30%|██▉       | 151M/507M [00:27<01:11, 5.20MB/s]go-1]<stderr>:\n",
      " 30%|██▉       | 152M/507M [00:27<01:14, 5.00MB/s]go-1]<stderr>:\n",
      " 30%|███       | 153M/507M [00:27<01:10, 5.28MB/s]go-1]<stderr>:\n",
      " 30%|███       | 153M/507M [00:27<01:13, 5.05MB/s]go-1]<stderr>:\n",
      " 30%|███       | 154M/507M [00:27<01:09, 5.31MB/s]go-1]<stderr>:\n",
      " 30%|███       | 154M/507M [00:27<01:12, 5.10MB/s]go-1]<stderr>:\n",
      " 31%|███       | 155M/507M [00:27<01:08, 5.37MB/s]go-1]<stderr>:\n",
      " 31%|███       | 155M/507M [00:27<01:11, 5.13MB/s]go-1]<stderr>:\n",
      " 31%|███       | 156M/507M [00:27<01:07, 5.42MB/s]go-1]<stderr>:\n",
      " 31%|███       | 156M/507M [00:28<01:10, 5.18MB/s]go-1]<stderr>:\n",
      " 31%|███       | 157M/507M [00:28<01:07, 5.43MB/s]go-1]<stderr>:\n",
      " 31%|███       | 158M/507M [00:28<01:10, 5.20MB/s]go-1]<stderr>:\n",
      " 31%|███       | 158M/507M [00:28<01:06, 5.51MB/s]go-1]<stderr>:\n",
      " 31%|███▏      | 159M/507M [00:28<01:09, 5.26MB/s]go-1]<stderr>:\n",
      " 31%|███▏      | 159M/507M [00:28<01:05, 5.56MB/s]go-1]<stderr>:\n",
      " 32%|███▏      | 160M/507M [00:28<01:08, 5.32MB/s]go-1]<stderr>:\n",
      " 32%|███▏      | 160M/507M [00:28<01:04, 5.62MB/s]go-1]<stderr>:\n",
      " 32%|███▏      | 161M/507M [00:28<01:07, 5.37MB/s]go-1]<stderr>:\n",
      " 32%|███▏      | 162M/507M [00:29<01:08, 5.31MB/s]go-1]<stderr>:\n",
      " 32%|███▏      | 162M/507M [00:29<01:04, 5.57MB/s]go-1]<stderr>:\n",
      " 32%|███▏      | 163M/507M [00:29<01:06, 5.42MB/s]go-1]<stderr>:\n",
      " 32%|███▏      | 163M/507M [00:29<01:03, 5.71MB/s]go-1]<stderr>:\n",
      " 32%|███▏      | 164M/507M [00:29<01:05, 5.53MB/s]go-1]<stderr>:\n",
      " 32%|███▏      | 165M/507M [00:29<01:02, 5.78MB/s]go-1]<stderr>:\n",
      " 33%|███▎      | 165M/507M [00:29<01:04, 5.58MB/s]go-1]<stderr>:\n",
      " 33%|███▎      | 166M/507M [00:29<01:00, 5.92MB/s]go-1]<stderr>:\n",
      " 33%|███▎      | 166M/507M [00:29<01:02, 5.69MB/s]go-1]<stderr>:\n",
      " 33%|███▎      | 167M/507M [00:30<00:59, 6.01MB/s]go-1]<stderr>:\n",
      " 33%|███▎      | 168M/507M [00:30<01:01, 5.75MB/s]go-1]<stderr>:\n",
      " 33%|███▎      | 168M/507M [00:30<00:58, 6.07MB/s]go-1]<stderr>:\n",
      " 33%|███▎      | 169M/507M [00:30<01:00, 5.85MB/s]go-1]<stderr>:\n",
      " 33%|███▎      | 169M/507M [00:30<00:57, 6.20MB/s]go-1]<stderr>:\n",
      " 34%|███▎      | 170M/507M [00:30<00:59, 5.95MB/s]go-1]<stderr>:\n",
      " 34%|███▎      | 171M/507M [00:30<00:55, 6.31MB/s]go-1]<stderr>:\n",
      " 34%|███▍      | 171M/507M [00:30<00:58, 6.04MB/s]go-1]<stderr>:\n",
      " 34%|███▍      | 172M/507M [00:30<00:54, 6.42MB/s]go-1]<stderr>:\n",
      " 34%|███▍      | 173M/507M [00:31<00:56, 6.18MB/s]go-1]<stderr>:\n",
      " 34%|███▍      | 173M/507M [00:31<00:53, 6.55MB/s]go-1]<stderr>:\n",
      " 34%|███▍      | 174M/507M [00:31<00:55, 6.29MB/s]go-1]<stderr>:\n",
      " 34%|███▍      | 175M/507M [00:31<00:52, 6.62MB/s]go-1]<stderr>:\n",
      " 35%|███▍      | 175M/507M [00:31<00:54, 6.41MB/s]go-1]<stderr>:\n",
      " 35%|███▍      | 176M/507M [00:31<00:51, 6.76MB/s]go-1]<stderr>:\n",
      " 35%|███▍      | 177M/507M [00:31<00:52, 6.54MB/s]go-1]<stderr>:\n",
      " 35%|███▌      | 178M/507M [00:31<00:50, 6.88MB/s]go-1]<stderr>:\n",
      " 35%|███▌      | 178M/507M [00:31<00:51, 6.65MB/s]go-1]<stderr>:\n",
      " 35%|███▌      | 179M/507M [00:31<00:49, 7.01MB/s]go-1]<stderr>:\n",
      " 35%|███▌      | 180M/507M [00:32<00:50, 6.75MB/s]go-1]<stderr>:\n",
      " 36%|███▌      | 180M/507M [00:32<00:47, 7.17MB/s]go-1]<stderr>:\n",
      " 36%|███▌      | 181M/507M [00:32<00:49, 6.92MB/s]go-1]<stderr>:\n",
      " 36%|███▌      | 182M/507M [00:32<00:46, 7.34MB/s]go-1]<stderr>:\n",
      " 36%|███▌      | 183M/507M [00:32<00:47, 7.09MB/s]go-1]<stderr>:\n",
      " 36%|███▌      | 184M/507M [00:32<00:45, 7.51MB/s]go-1]<stderr>:\n",
      " 36%|███▋      | 184M/507M [00:32<00:46, 7.26MB/s]go-1]<stderr>:\n",
      " 37%|███▋      | 185M/507M [00:32<00:44, 7.65MB/s]go-1]<stderr>:\n",
      " 37%|███▋      | 186M/507M [00:32<00:45, 7.41MB/s]go-1]<stderr>:\n",
      " 37%|███▋      | 187M/507M [00:33<00:43, 7.81MB/s]go-1]<stderr>:\n",
      " 37%|███▋      | 187M/507M [00:33<00:44, 7.56MB/s]go-1]<stderr>:\n",
      " 37%|███▋      | 188M/507M [00:33<00:44, 7.49MB/s]go-1]<stderr>:\n",
      " 37%|███▋      | 189M/507M [00:33<00:42, 7.93MB/s]go-1]<stderr>:\n",
      " 37%|███▋      | 190M/507M [00:33<00:42, 7.76MB/s]go-1]<stderr>:\n",
      " 38%|███▊      | 191M/507M [00:33<00:41, 8.05MB/s]go-1]<stderr>:\n",
      " 38%|███▊      | 192M/507M [00:33<00:42, 7.87MB/s]go-1]<stderr>:\n",
      " 38%|███▊      | 192M/507M [00:33<00:40, 8.20MB/s]go-1]<stderr>:\n",
      " 38%|███▊      | 193M/507M [00:33<00:41, 7.96MB/s]go-1]<stderr>:\n",
      " 38%|███▊      | 194M/507M [00:34<00:39, 8.30MB/s]go-1]<stderr>:\n",
      " 38%|███▊      | 195M/507M [00:34<00:40, 8.02MB/s]go-1]<stderr>:\n",
      " 39%|███▊      | 196M/507M [00:34<00:39, 8.36MB/s]go-1]<stderr>:\n",
      " 39%|███▉      | 197M/507M [00:34<00:40, 8.04MB/s]go-1]<stderr>:\n",
      " 39%|███▉      | 197M/507M [00:34<00:38, 8.40MB/s]go-1]<stderr>:\n",
      " 39%|███▉      | 198M/507M [00:34<00:40, 8.07MB/s]go-1]<stderr>:\n",
      " 39%|███▉      | 199M/507M [00:34<00:38, 8.44MB/s]go-1]<stderr>:\n",
      " 39%|███▉      | 200M/507M [00:34<00:39, 8.13MB/s]go-1]<stderr>:\n",
      " 40%|███▉      | 201M/507M [00:34<00:37, 8.47MB/s]go-1]<stderr>:\n",
      " 40%|███▉      | 202M/507M [00:35<00:39, 8.14MB/s]go-1]<stderr>:\n",
      " 40%|███▉      | 203M/507M [00:35<00:37, 8.51MB/s]go-1]<stderr>:\n",
      " 40%|████      | 203M/507M [00:35<00:38, 8.18MB/s]go-1]<stderr>:\n",
      " 40%|████      | 204M/507M [00:35<00:37, 8.53MB/s]go-1]<stderr>:\n",
      " 40%|████      | 205M/507M [00:35<00:38, 8.22MB/s]go-1]<stderr>:\n",
      " 41%|████      | 206M/507M [00:35<00:36, 8.55MB/s]go-1]<stderr>:\n",
      " 41%|████      | 207M/507M [00:35<00:38, 8.20MB/s]go-1]<stderr>:\n",
      " 41%|████      | 208M/507M [00:35<00:36, 8.55MB/s]go-1]<stderr>:\n",
      " 41%|████      | 209M/507M [00:35<00:38, 8.14MB/s]go-1]<stderr>:\n",
      " 41%|████▏     | 210M/507M [00:35<00:36, 8.58MB/s]go-1]<stderr>:\n",
      " 42%|████▏     | 210M/507M [00:36<00:38, 8.16MB/s]go-1]<stderr>:\n",
      " 42%|████▏     | 211M/507M [00:36<00:36, 8.60MB/s]go-1]<stderr>:\n",
      " 42%|████▏     | 212M/507M [00:36<00:37, 8.21MB/s]go-1]<stderr>:\n",
      " 42%|████▏     | 213M/507M [00:36<00:35, 8.60MB/s]go-1]<stderr>:\n",
      " 42%|████▏     | 214M/507M [00:36<00:37, 8.23MB/s]go-1]<stderr>:\n",
      " 42%|████▏     | 215M/507M [00:36<00:35, 8.60MB/s]go-1]<stderr>:\n",
      " 43%|████▎     | 216M/507M [00:36<00:37, 8.23MB/s]go-1]<stderr>:\n",
      " 43%|████▎     | 217M/507M [00:36<00:35, 8.59MB/s]go-1]<stderr>:\n",
      " 43%|████▎     | 217M/507M [00:36<00:36, 8.25MB/s]go-1]<stderr>:\n",
      " 43%|████▎     | 218M/507M [00:37<00:37, 8.07MB/s]go-1]<stderr>:\n",
      " 43%|████▎     | 219M/507M [00:37<00:35, 8.40MB/s]go-1]<stderr>:\n",
      " 43%|████▎     | 220M/507M [00:37<00:36, 8.17MB/s]go-1]<stderr>:\n",
      " 44%|████▎     | 221M/507M [00:37<00:35, 8.52MB/s]go-1]<stderr>:\n",
      " 44%|████▎     | 222M/507M [00:37<00:36, 8.22MB/s]go-1]<stderr>:\n",
      " 44%|████▍     | 223M/507M [00:37<00:34, 8.56MB/s]go-1]<stderr>:\n",
      " 44%|████▍     | 223M/507M [00:37<00:36, 8.23MB/s]go-1]<stderr>:\n",
      " 44%|████▍     | 224M/507M [00:37<00:34, 8.63MB/s]go-1]<stderr>:\n",
      " 44%|████▍     | 225M/507M [00:37<00:35, 8.25MB/s]go-1]<stderr>:\n",
      " 45%|████▍     | 226M/507M [00:38<00:33, 8.70MB/s]go-1]<stderr>:\n",
      " 45%|████▍     | 227M/507M [00:38<00:35, 8.28MB/s]go-1]<stderr>:\n",
      " 45%|████▍     | 228M/507M [00:38<00:33, 8.73MB/s]go-1]<stderr>:\n",
      " 45%|████▌     | 229M/507M [00:38<00:34, 8.34MB/s]go-1]<stderr>:\n",
      " 45%|████▌     | 230M/507M [00:38<00:33, 8.74MB/s]go-1]<stderr>:\n",
      " 45%|████▌     | 231M/507M [00:38<00:34, 8.39MB/s]go-1]<stderr>:\n",
      " 46%|████▌     | 232M/507M [00:38<00:33, 8.74MB/s]go-1]<stderr>:\n",
      " 46%|████▌     | 232M/507M [00:38<00:34, 8.42MB/s]go-1]<stderr>:\n",
      " 46%|████▌     | 233M/507M [00:38<00:32, 8.77MB/s]go-1]<stderr>:\n",
      " 46%|████▌     | 234M/507M [00:39<00:34, 8.41MB/s]go-1]<stderr>:\n",
      " 46%|████▋     | 235M/507M [00:39<00:32, 8.84MB/s]go-1]<stderr>:\n",
      " 47%|████▋     | 236M/507M [00:39<00:33, 8.44MB/s]go-1]<stderr>:\n",
      " 47%|████▋     | 237M/507M [00:39<00:31, 8.89MB/s]go-1]<stderr>:\n",
      " 47%|████▋     | 238M/507M [00:39<00:33, 8.45MB/s]go-1]<stderr>:\n",
      " 47%|████▋     | 239M/507M [00:39<00:31, 8.87MB/s]go-1]<stderr>:\n",
      " 47%|████▋     | 240M/507M [00:39<00:33, 8.48MB/s]go-1]<stderr>:\n",
      " 47%|████▋     | 241M/507M [00:39<00:31, 8.86MB/s]go-1]<stderr>:\n",
      " 48%|████▊     | 241M/507M [00:39<00:32, 8.50MB/s]go-1]<stderr>:\n",
      " 48%|████▊     | 242M/507M [00:40<00:31, 8.84MB/s]go-1]<stderr>:\n",
      " 48%|████▊     | 243M/507M [00:40<00:32, 8.57MB/s]go-1]<stderr>:\n",
      " 48%|████▊     | 244M/507M [00:40<00:32, 8.44MB/s]go-1]<stderr>:\n",
      " 48%|████▊     | 245M/507M [00:40<00:31, 8.70MB/s]go-1]<stderr>:\n",
      " 49%|████▊     | 246M/507M [00:40<00:32, 8.52MB/s]go-1]<stderr>:\n",
      " 49%|████▊     | 247M/507M [00:40<00:31, 8.79MB/s]go-1]<stderr>:\n",
      " 49%|████▉     | 248M/507M [00:40<00:31, 8.55MB/s]go-1]<stderr>:\n",
      " 49%|████▉     | 249M/507M [00:40<00:30, 8.80MB/s]go-1]<stderr>:\n",
      " 49%|████▉     | 249M/507M [00:40<00:31, 8.65MB/s]go-1]<stderr>:\n",
      " 49%|████▉     | 250M/507M [00:40<00:30, 8.87MB/s]go-1]<stderr>:\n",
      " 50%|████▉     | 251M/507M [00:41<00:30, 8.73MB/s]go-1]<stderr>:\n",
      " 50%|████▉     | 252M/507M [00:41<00:30, 8.88MB/s]go-1]<stderr>:\n",
      " 50%|████▉     | 253M/507M [00:41<00:30, 8.71MB/s]go-1]<stderr>:\n",
      " 50%|█████     | 254M/507M [00:41<00:29, 8.93MB/s]go-1]<stderr>:\n",
      " 50%|█████     | 255M/507M [00:41<00:30, 8.77MB/s]go-1]<stderr>:\n",
      " 50%|█████     | 256M/507M [00:41<00:29, 8.96MB/s]go-1]<stderr>:\n",
      " 51%|█████     | 257M/507M [00:41<00:29, 8.80MB/s]go-1]<stderr>:\n",
      " 51%|█████     | 257M/507M [00:41<00:29, 8.97MB/s]go-1]<stderr>:\n",
      " 51%|█████     | 258M/507M [00:41<00:29, 8.82MB/s]go-1]<stderr>:\n",
      " 51%|█████     | 259M/507M [00:42<00:28, 8.96MB/s]go-1]<stderr>:\n",
      " 51%|█████▏    | 260M/507M [00:42<00:29, 8.81MB/s]go-1]<stderr>:\n",
      " 51%|█████▏    | 261M/507M [00:42<00:28, 8.98MB/s]go-1]<stderr>:\n",
      " 52%|█████▏    | 262M/507M [00:42<00:29, 8.81MB/s]go-1]<stderr>:\n",
      " 52%|█████▏    | 263M/507M [00:42<00:28, 8.97MB/s]go-1]<stderr>:\n",
      " 52%|█████▏    | 264M/507M [00:42<00:28, 8.85MB/s]go-1]<stderr>:\n",
      " 52%|█████▏    | 265M/507M [00:42<00:28, 9.01MB/s]go-1]<stderr>:\n",
      " 52%|█████▏    | 265M/507M [00:42<00:28, 8.86MB/s]go-1]<stderr>:\n",
      " 53%|█████▎    | 266M/507M [00:42<00:27, 9.03MB/s]go-1]<stderr>:\n",
      " 53%|█████▎    | 267M/507M [00:42<00:28, 8.86MB/s]go-1]<stderr>:\n",
      " 53%|█████▎    | 268M/507M [00:43<00:27, 9.04MB/s]go-1]<stderr>:\n",
      " 53%|█████▎    | 269M/507M [00:43<00:28, 8.88MB/s]go-1]<stderr>:\n",
      " 53%|█████▎    | 270M/507M [00:43<00:27, 9.01MB/s]go-1]<stderr>:\n",
      " 53%|█████▎    | 271M/507M [00:43<00:27, 8.91MB/s]go-1]<stderr>:\n",
      " 54%|█████▎    | 272M/507M [00:43<00:27, 9.03MB/s]go-1]<stderr>:\n",
      " 54%|█████▍    | 273M/507M [00:43<00:27, 8.89MB/s]go-1]<stderr>:\n",
      " 54%|█████▍    | 274M/507M [00:43<00:27, 9.03MB/s]go-1]<stderr>:\n",
      " 54%|█████▍    | 274M/507M [00:43<00:27, 8.86MB/s]go-1]<stderr>:\n",
      " 54%|█████▍    | 275M/507M [00:43<00:26, 9.08MB/s]go-1]<stderr>:\n",
      " 55%|█████▍    | 276M/507M [00:44<00:27, 8.92MB/s]go-1]<stderr>:\n",
      " 55%|█████▍    | 277M/507M [00:44<00:26, 9.08MB/s]go-1]<stderr>:\n",
      " 55%|█████▍    | 278M/507M [00:44<00:26, 8.93MB/s]go-1]<stderr>:\n",
      " 55%|█████▌    | 279M/507M [00:44<00:26, 9.07MB/s]go-1]<stderr>:\n",
      " 55%|█████▌    | 280M/507M [00:44<00:26, 8.95MB/s]go-1]<stderr>:\n",
      " 55%|█████▌    | 281M/507M [00:44<00:26, 9.08MB/s]go-1]<stderr>:\n",
      " 56%|█████▌    | 282M/507M [00:44<00:26, 8.95MB/s]go-1]<stderr>:\n",
      " 56%|█████▌    | 283M/507M [00:44<00:25, 9.14MB/s]go-1]<stderr>:\n",
      " 56%|█████▌    | 284M/507M [00:44<00:26, 8.99MB/s]go-1]<stderr>:\n",
      " 56%|█████▌    | 284M/507M [00:44<00:25, 9.17MB/s]go-1]<stderr>:\n",
      " 56%|█████▋    | 285M/507M [00:45<00:25, 8.98MB/s]go-1]<stderr>:\n",
      " 56%|█████▋    | 286M/507M [00:45<00:25, 9.14MB/s]go-1]<stderr>:\n",
      " 57%|█████▋    | 287M/507M [00:45<00:25, 8.98MB/s]go-1]<stderr>:\n",
      " 57%|█████▋    | 288M/507M [00:45<00:25, 9.14MB/s]go-1]<stderr>:\n",
      " 57%|█████▋    | 289M/507M [00:45<00:25, 9.00MB/s]go-1]<stderr>:\n",
      " 57%|█████▋    | 290M/507M [00:45<00:24, 9.14MB/s]go-1]<stderr>:\n",
      " 57%|█████▋    | 291M/507M [00:45<00:25, 9.02MB/s]go-1]<stderr>:\n",
      " 58%|█████▊    | 292M/507M [00:45<00:24, 9.18MB/s]go-1]<stderr>:\n",
      " 58%|█████▊    | 293M/507M [00:45<00:24, 9.12MB/s]go-1]<stderr>:\n",
      " 58%|█████▊    | 294M/507M [00:46<00:24, 9.24MB/s]go-1]<stderr>:\n",
      " 58%|█████▊    | 295M/507M [00:46<00:24, 9.11MB/s]go-1]<stderr>:\n",
      " 58%|█████▊    | 295M/507M [00:46<00:24, 9.23MB/s]go-1]<stderr>:\n",
      " 58%|█████▊    | 296M/507M [00:46<00:24, 9.12MB/s]go-1]<stderr>:\n",
      " 59%|█████▊    | 297M/507M [00:46<00:23, 9.26MB/s]go-1]<stderr>:\n",
      " 59%|█████▉    | 298M/507M [00:46<00:23, 9.16MB/s]go-1]<stderr>:\n",
      " 59%|█████▉    | 299M/507M [00:46<00:23, 9.29MB/s]go-1]<stderr>:\n",
      " 59%|█████▉    | 300M/507M [00:46<00:23, 9.18MB/s]go-1]<stderr>:\n",
      " 59%|█████▉    | 301M/507M [00:46<00:23, 9.30MB/s]go-1]<stderr>:\n",
      " 60%|█████▉    | 302M/507M [00:46<00:23, 9.24MB/s]go-1]<stderr>:\n",
      " 60%|█████▉    | 303M/507M [00:47<00:22, 9.31MB/s]go-1]<stderr>:\n",
      " 60%|█████▉    | 304M/507M [00:47<00:23, 9.23MB/s]go-1]<stderr>:\n",
      " 60%|██████    | 305M/507M [00:47<00:22, 9.28MB/s]go-1]<stderr>:\n",
      " 60%|██████    | 306M/507M [00:47<00:22, 9.21MB/s]go-1]<stderr>:\n",
      " 61%|██████    | 307M/507M [00:47<00:22, 9.33MB/s]go-1]<stderr>:\n",
      " 61%|██████    | 308M/507M [00:47<00:22, 9.25MB/s]go-1]<stderr>:\n",
      " 61%|██████    | 308M/507M [00:47<00:22, 9.32MB/s]go-1]<stderr>:\n",
      " 61%|██████    | 309M/507M [00:47<00:22, 9.30MB/s]go-1]<stderr>:\n",
      " 61%|██████    | 310M/507M [00:47<00:22, 9.36MB/s]go-1]<stderr>:\n",
      " 61%|██████▏   | 311M/507M [00:48<00:22, 9.23MB/s]go-1]<stderr>:\n",
      " 62%|██████▏   | 312M/507M [00:48<00:21, 9.39MB/s]go-1]<stderr>:\n",
      " 62%|██████▏   | 313M/507M [00:48<00:21, 9.32MB/s]go-1]<stderr>:\n",
      " 62%|██████▏   | 314M/507M [00:48<00:21, 9.40MB/s]go-1]<stderr>:\n",
      " 62%|██████▏   | 315M/507M [00:48<00:21, 9.35MB/s]go-1]<stderr>:\n",
      " 62%|██████▏   | 316M/507M [00:48<00:21, 9.47MB/s]go-1]<stderr>:\n",
      " 63%|██████▎   | 317M/507M [00:48<00:21, 9.40MB/s]go-1]<stderr>:\n",
      " 63%|██████▎   | 318M/507M [00:48<00:20, 9.48MB/s]go-1]<stderr>:\n",
      " 63%|██████▎   | 319M/507M [00:48<00:21, 9.33MB/s]go-1]<stderr>:\n",
      " 63%|██████▎   | 320M/507M [00:48<00:20, 9.53MB/s]go-1]<stderr>:\n",
      " 63%|██████▎   | 321M/507M [00:49<00:20, 9.54MB/s]go-1]<stderr>:\n",
      " 63%|██████▎   | 322M/507M [00:49<00:20, 9.58MB/s]go-1]<stderr>:\n",
      " 64%|██████▎   | 323M/507M [00:49<00:20, 9.57MB/s]go-1]<stderr>:\n",
      " 64%|██████▍   | 324M/507M [00:49<00:20, 9.57MB/s]go-1]<stderr>:\n",
      " 64%|██████▍   | 325M/507M [00:49<00:19, 9.57MB/s]go-1]<stderr>:\n",
      " 64%|██████▍   | 326M/507M [00:49<00:19, 9.55MB/s]go-1]<stderr>:\n",
      " 64%|██████▍   | 327M/507M [00:49<00:19, 9.62MB/s]go-1]<stderr>:\n",
      " 65%|██████▍   | 328M/507M [00:49<00:19, 9.61MB/s]go-1]<stderr>:\n",
      " 65%|██████▍   | 329M/507M [00:49<00:19, 9.64MB/s]go-1]<stderr>:\n",
      " 65%|██████▌   | 330M/507M [00:50<00:19, 9.62MB/s]go-1]<stderr>:\n",
      " 65%|██████▌   | 331M/507M [00:50<00:19, 9.67MB/s]go-1]<stderr>:\n",
      " 65%|██████▌   | 331M/507M [00:50<00:19, 9.62MB/s]go-1]<stderr>:\n",
      " 66%|██████▌   | 332M/507M [00:50<00:18, 9.74MB/s]go-1]<stderr>:\n",
      " 66%|██████▌   | 333M/507M [00:50<00:18, 9.66MB/s]go-1]<stderr>:\n",
      " 66%|██████▌   | 334M/507M [00:50<00:18, 9.73MB/s]go-1]<stderr>:\n",
      " 66%|██████▌   | 335M/507M [00:50<00:18, 9.65MB/s]go-1]<stderr>:\n",
      " 66%|██████▋   | 336M/507M [00:50<00:18, 9.75MB/s]go-1]<stderr>:\n",
      " 67%|██████▋   | 337M/507M [00:50<00:18, 9.72MB/s]go-1]<stderr>:\n",
      " 67%|██████▋   | 338M/507M [00:50<00:18, 9.81MB/s]go-1]<stderr>:\n",
      " 67%|██████▋   | 339M/507M [00:51<00:18, 9.73MB/s]go-1]<stderr>:\n",
      " 67%|██████▋   | 340M/507M [00:51<00:17, 9.78MB/s]go-1]<stderr>:\n",
      " 67%|██████▋   | 341M/507M [00:51<00:17, 9.79MB/s]go-1]<stderr>:\n",
      " 68%|██████▊   | 342M/507M [00:51<00:17, 9.84MB/s]go-1]<stderr>:\n",
      " 68%|██████▊   | 343M/507M [00:51<00:17, 9.85MB/s]go-1]<stderr>:\n",
      " 68%|██████▊   | 344M/507M [00:51<00:17, 9.83MB/s]go-1]<stderr>:\n",
      " 68%|██████▊   | 345M/507M [00:51<00:17, 9.83MB/s]go-1]<stderr>:\n",
      " 68%|██████▊   | 346M/507M [00:51<00:17, 9.86MB/s]go-1]<stderr>:\n",
      " 69%|██████▊   | 347M/507M [00:51<00:16, 9.85MB/s]go-1]<stderr>:\n",
      " 69%|██████▊   | 348M/507M [00:52<00:16, 9.93MB/s]go-1]<stderr>:\n",
      " 69%|██████▉   | 349M/507M [00:52<00:16, 9.82MB/s]go-1]<stderr>:\n",
      " 69%|██████▉   | 350M/507M [00:52<00:16, 9.93MB/s]go-1]<stderr>:\n",
      " 69%|██████▉   | 351M/507M [00:52<00:16, 9.87MB/s]go-1]<stderr>:\n",
      " 69%|██████▉   | 352M/507M [00:52<00:16, 9.94MB/s]go-1]<stderr>:\n",
      " 70%|██████▉   | 353M/507M [00:52<00:16, 9.90MB/s]go-1]<stderr>:\n",
      " 70%|██████▉   | 354M/507M [00:52<00:15, 10.0MB/s]go-1]<stderr>:\n",
      " 70%|███████   | 355M/507M [00:52<00:15, 9.98MB/s]go-1]<stderr>:\n",
      " 70%|███████   | 356M/507M [00:52<00:15, 10.0MB/s]go-1]<stderr>:\n",
      " 70%|███████   | 357M/507M [00:52<00:15, 10.0MB/s]go-1]<stderr>:\n",
      " 71%|███████   | 358M/507M [00:53<00:15, 10.0MB/s]go-1]<stderr>:\n",
      " 71%|███████   | 359M/507M [00:53<00:15, 9.99MB/s]go-1]<stderr>:\n",
      " 71%|███████   | 360M/507M [00:53<00:15, 10.1MB/s]go-1]<stderr>:\n",
      " 71%|███████▏  | 361M/507M [00:53<00:15, 10.1MB/s]go-1]<stderr>:\n",
      " 71%|███████▏  | 362M/507M [00:53<00:15, 10.1MB/s]go-1]<stderr>:\n",
      " 72%|███████▏  | 363M/507M [00:53<00:14, 10.1MB/s]go-1]<stderr>:\n",
      " 72%|███████▏  | 364M/507M [00:53<00:14, 10.2MB/s]go-1]<stderr>:\n",
      " 72%|███████▏  | 365M/507M [00:53<00:14, 10.1MB/s]go-1]<stderr>:\n",
      " 72%|███████▏  | 366M/507M [00:53<00:14, 10.2MB/s]go-1]<stderr>:\n",
      " 72%|███████▏  | 367M/507M [00:54<00:14, 10.1MB/s]go-1]<stderr>:\n",
      " 73%|███████▎  | 368M/507M [00:54<00:14, 10.2MB/s]go-1]<stderr>:\n",
      " 73%|███████▎  | 369M/507M [00:54<00:14, 10.1MB/s]go-1]<stderr>:\n",
      " 73%|███████▎  | 370M/507M [00:54<00:14, 10.2MB/s]go-1]<stderr>:\n",
      " 73%|███████▎  | 371M/507M [00:54<00:14, 10.1MB/s]go-1]<stderr>:\n",
      " 74%|███████▎  | 373M/507M [00:54<00:13, 10.2MB/s]go-1]<stderr>:\n",
      " 74%|███████▎  | 374M/507M [00:54<00:13, 10.2MB/s]go-1]<stderr>:\n",
      " 74%|███████▍  | 375M/507M [00:54<00:13, 10.2MB/s]go-1]<stderr>:\n",
      " 74%|███████▍  | 376M/507M [00:54<00:13, 10.2MB/s]go-1]<stderr>:\n",
      " 74%|███████▍  | 377M/507M [00:54<00:13, 10.3MB/s]go-1]<stderr>:\n",
      " 75%|███████▍  | 378M/507M [00:55<00:13, 10.2MB/s]go-1]<stderr>:\n",
      " 75%|███████▍  | 379M/507M [00:55<00:13, 10.3MB/s]go-1]<stderr>:\n",
      " 75%|███████▍  | 380M/507M [00:55<00:13, 10.2MB/s]go-1]<stderr>:\n",
      " 75%|███████▌  | 381M/507M [00:55<00:12, 10.3MB/s]go-1]<stderr>:\n",
      " 75%|███████▌  | 382M/507M [00:55<00:12, 10.2MB/s]go-1]<stderr>:\n",
      " 76%|███████▌  | 383M/507M [00:55<00:12, 10.3MB/s]go-1]<stderr>:\n",
      " 76%|███████▌  | 384M/507M [00:55<00:12, 10.2MB/s]go-1]<stderr>:\n",
      " 76%|███████▌  | 385M/507M [00:55<00:12, 10.3MB/s]go-1]<stderr>:\n",
      " 76%|███████▌  | 386M/507M [00:55<00:12, 10.2MB/s]go-1]<stderr>:\n",
      " 76%|███████▋  | 387M/507M [00:56<00:12, 10.2MB/s]go-1]<stderr>:\n",
      " 77%|███████▋  | 388M/507M [00:56<00:12, 10.3MB/s]go-1]<stderr>:\n",
      " 77%|███████▋  | 389M/507M [00:56<00:11, 10.3MB/s]go-1]<stderr>:\n",
      " 77%|███████▋  | 390M/507M [00:56<00:11, 10.3MB/s]go-1]<stderr>:\n",
      " 77%|███████▋  | 391M/507M [00:56<00:11, 10.3MB/s]go-1]<stderr>:\n",
      " 77%|███████▋  | 392M/507M [00:56<00:11, 10.3MB/s]go-1]<stderr>:\n",
      " 78%|███████▊  | 393M/507M [00:56<00:11, 10.3MB/s]go-1]<stderr>:\n",
      " 78%|███████▊  | 394M/507M [00:56<00:11, 10.3MB/s]go-1]<stderr>:\n",
      " 78%|███████▊  | 395M/507M [00:56<00:11, 10.5MB/s]go-1]<stderr>:\n",
      " 78%|███████▊  | 396M/507M [00:56<00:11, 10.3MB/s]go-1]<stderr>:\n",
      " 78%|███████▊  | 397M/507M [00:57<00:10, 10.4MB/s]go-1]<stderr>:\n",
      " 79%|███████▊  | 398M/507M [00:57<00:11, 10.3MB/s]go-1]<stderr>:\n",
      " 79%|███████▉  | 399M/507M [00:57<00:10, 10.5MB/s]go-1]<stderr>:\n",
      " 79%|███████▉  | 400M/507M [00:57<00:10, 10.4MB/s]go-1]<stderr>:\n",
      " 79%|███████▉  | 402M/507M [00:57<00:10, 10.5MB/s]go-1]<stderr>:\n",
      " 79%|███████▉  | 403M/507M [00:57<00:10, 10.4MB/s]go-1]<stderr>:\n",
      " 80%|███████▉  | 404M/507M [00:57<00:10, 10.5MB/s]go-1]<stderr>:\n",
      " 80%|███████▉  | 405M/507M [00:57<00:10, 10.4MB/s]go-1]<stderr>:\n",
      " 80%|████████  | 406M/507M [00:57<00:10, 10.5MB/s]go-1]<stderr>:\n",
      " 80%|████████  | 407M/507M [00:58<00:10, 10.4MB/s]go-1]<stderr>:\n",
      " 80%|████████  | 408M/507M [00:58<00:09, 10.5MB/s]go-1]<stderr>:\n",
      " 81%|████████  | 409M/507M [00:58<00:09, 10.4MB/s]go-1]<stderr>:\n",
      " 81%|████████  | 410M/507M [00:58<00:09, 10.6MB/s]go-1]<stderr>:\n",
      " 81%|████████  | 411M/507M [00:58<00:09, 10.5MB/s]go-1]<stderr>:\n",
      " 81%|████████▏ | 412M/507M [00:58<00:09, 10.6MB/s]go-1]<stderr>:\n",
      " 82%|████████▏ | 413M/507M [00:58<00:09, 10.5MB/s]go-1]<stderr>:\n",
      " 82%|████████▏ | 414M/507M [00:58<00:09, 10.6MB/s]go-1]<stderr>:\n",
      " 82%|████████▏ | 415M/507M [00:58<00:09, 10.6MB/s]go-1]<stderr>:\n",
      " 82%|████████▏ | 416M/507M [00:58<00:08, 10.7MB/s]go-1]<stderr>:\n",
      " 82%|████████▏ | 417M/507M [00:59<00:08, 10.6MB/s]go-1]<stderr>:\n",
      " 83%|████████▎ | 419M/507M [00:59<00:08, 10.8MB/s]go-1]<stderr>:\n",
      " 83%|████████▎ | 420M/507M [00:59<00:08, 10.6MB/s]go-1]<stderr>:\n",
      " 83%|████████▎ | 421M/507M [00:59<00:08, 10.8MB/s]go-1]<stderr>:\n",
      " 83%|████████▎ | 422M/507M [00:59<00:08, 10.7MB/s]go-1]<stderr>:\n",
      " 83%|████████▎ | 423M/507M [00:59<00:08, 10.8MB/s]go-1]<stderr>:\n",
      " 84%|████████▎ | 424M/507M [00:59<00:08, 10.7MB/s]go-1]<stderr>:\n",
      " 84%|████████▍ | 425M/507M [00:59<00:07, 10.8MB/s]go-1]<stderr>:\n",
      " 84%|████████▍ | 426M/507M [00:59<00:07, 10.7MB/s]go-1]<stderr>:\n",
      " 84%|████████▍ | 427M/507M [01:00<00:07, 10.9MB/s]go-1]<stderr>:\n",
      " 84%|████████▍ | 428M/507M [01:00<00:07, 10.7MB/s]go-1]<stderr>:\n",
      " 85%|████████▍ | 429M/507M [01:00<00:07, 10.9MB/s]go-1]<stderr>:\n",
      " 85%|████████▍ | 430M/507M [01:00<00:07, 10.7MB/s]go-1]<stderr>:\n",
      " 85%|████████▌ | 432M/507M [01:00<00:07, 10.8MB/s]go-1]<stderr>:\n",
      " 85%|████████▌ | 433M/507M [01:00<00:07, 10.7MB/s]go-1]<stderr>:\n",
      " 86%|████████▌ | 434M/507M [01:00<00:07, 10.9MB/s]go-1]<stderr>:\n",
      " 86%|████████▌ | 435M/507M [01:00<00:07, 10.8MB/s]go-1]<stderr>:\n",
      " 86%|████████▌ | 436M/507M [01:00<00:06, 10.9MB/s]go-1]<stderr>:\n",
      " 86%|████████▌ | 437M/507M [01:00<00:06, 10.8MB/s]go-1]<stderr>:\n",
      " 86%|████████▋ | 438M/507M [01:01<00:06, 10.9MB/s]go-1]<stderr>:\n",
      " 87%|████████▋ | 439M/507M [01:01<00:06, 10.8MB/s]go-1]<stderr>:\n",
      " 87%|████████▋ | 440M/507M [01:01<00:06, 10.9MB/s]go-1]<stderr>:\n",
      " 87%|████████▋ | 441M/507M [01:01<00:06, 10.8MB/s]go-1]<stderr>:\n",
      " 87%|████████▋ | 442M/507M [01:01<00:06, 11.0MB/s]go-1]<stderr>:\n",
      " 88%|████████▊ | 444M/507M [01:01<00:06, 10.8MB/s]go-1]<stderr>:\n",
      " 88%|████████▊ | 445M/507M [01:01<00:05, 11.0MB/s]go-1]<stderr>:\n",
      " 88%|████████▊ | 446M/507M [01:01<00:05, 10.8MB/s]go-1]<stderr>:\n",
      " 88%|████████▊ | 447M/507M [01:01<00:05, 11.0MB/s]go-1]<stderr>:\n",
      " 88%|████████▊ | 448M/507M [01:02<00:05, 10.8MB/s]go-1]<stderr>:\n",
      " 89%|████████▊ | 449M/507M [01:02<00:05, 11.1MB/s]go-1]<stderr>:\n",
      " 89%|████████▉ | 450M/507M [01:02<00:05, 10.8MB/s]go-1]<stderr>:\n",
      " 89%|████████▉ | 451M/507M [01:02<00:05, 11.1MB/s]go-1]<stderr>:\n",
      " 89%|████████▉ | 452M/507M [01:02<00:05, 10.9MB/s]go-1]<stderr>:\n",
      " 89%|████████▉ | 453M/507M [01:02<00:05, 11.1MB/s]go-1]<stderr>:\n",
      " 90%|████████▉ | 455M/507M [01:02<00:05, 10.9MB/s]go-1]<stderr>:\n",
      " 90%|████████▉ | 456M/507M [01:02<00:04, 11.0MB/s]go-1]<stderr>:\n",
      " 90%|█████████ | 457M/507M [01:02<00:04, 10.9MB/s]go-1]<stderr>:\n",
      " 90%|█████████ | 458M/507M [01:02<00:04, 11.1MB/s]go-1]<stderr>:\n",
      " 91%|█████████ | 459M/507M [01:03<00:04, 10.9MB/s]go-1]<stderr>:\n",
      " 91%|█████████ | 460M/507M [01:03<00:04, 11.1MB/s]go-1]<stderr>:\n",
      " 91%|█████████ | 461M/507M [01:03<00:04, 10.9MB/s]go-1]<stderr>:\n",
      " 91%|█████████ | 462M/507M [01:03<00:04, 11.1MB/s]go-1]<stderr>:\n",
      " 91%|█████████▏| 463M/507M [01:03<00:04, 10.9MB/s]go-1]<stderr>:\n",
      " 92%|█████████▏| 465M/507M [01:03<00:03, 11.1MB/s]go-1]<stderr>:\n",
      " 92%|█████████▏| 466M/507M [01:03<00:03, 10.9MB/s]go-1]<stderr>:\n",
      " 92%|█████████▏| 467M/507M [01:03<00:03, 11.2MB/s]go-1]<stderr>:\n",
      " 92%|█████████▏| 468M/507M [01:03<00:03, 10.9MB/s]go-1]<stderr>:\n",
      " 93%|█████████▎| 469M/507M [01:04<00:03, 11.2MB/s]go-1]<stderr>:\n",
      " 93%|█████████▎| 470M/507M [01:04<00:03, 10.9MB/s]go-1]<stderr>:\n",
      " 93%|█████████▎| 471M/507M [01:04<00:03, 11.1MB/s]go-1]<stderr>:\n",
      " 93%|█████████▎| 472M/507M [01:04<00:03, 11.0MB/s]go-1]<stderr>:\n",
      " 93%|█████████▎| 473M/507M [01:04<00:03, 11.2MB/s]go-1]<stderr>:\n",
      " 94%|█████████▎| 475M/507M [01:04<00:03, 11.1MB/s]go-1]<stderr>:\n",
      " 94%|█████████▍| 476M/507M [01:04<00:02, 11.3MB/s]go-1]<stderr>:\n",
      " 94%|█████████▍| 477M/507M [01:04<00:02, 11.1MB/s]go-1]<stderr>:\n",
      " 94%|█████████▍| 478M/507M [01:04<00:02, 11.3MB/s]go-1]<stderr>:\n",
      " 95%|█████████▍| 479M/507M [01:04<00:02, 11.1MB/s]go-1]<stderr>:\n",
      " 95%|█████████▍| 480M/507M [01:05<00:02, 11.3MB/s]go-1]<stderr>:\n",
      " 95%|█████████▍| 481M/507M [01:05<00:02, 11.1MB/s]go-1]<stderr>:\n",
      " 95%|█████████▌| 482M/507M [01:05<00:02, 11.3MB/s]go-1]<stderr>:\n",
      " 95%|█████████▌| 484M/507M [01:05<00:02, 11.1MB/s]go-1]<stderr>:\n",
      " 96%|█████████▌| 485M/507M [01:05<00:02, 11.4MB/s]go-1]<stderr>:\n",
      " 96%|█████████▌| 486M/507M [01:05<00:01, 11.2MB/s]go-1]<stderr>:\n",
      " 96%|█████████▌| 487M/507M [01:05<00:01, 11.4MB/s]go-1]<stderr>:\n",
      " 96%|█████████▋| 488M/507M [01:05<00:01, 11.1MB/s]go-1]<stderr>:\n",
      " 97%|█████████▋| 489M/507M [01:05<00:01, 11.4MB/s]go-1]<stderr>:\n",
      " 97%|█████████▋| 490M/507M [01:06<00:01, 11.2MB/s]go-1]<stderr>:\n",
      " 97%|█████████▋| 492M/507M [01:06<00:01, 11.4MB/s]go-1]<stderr>:\n",
      " 97%|█████████▋| 493M/507M [01:06<00:01, 11.2MB/s]go-1]<stderr>:\n",
      " 97%|█████████▋| 494M/507M [01:06<00:01, 11.4MB/s]go-1]<stderr>:\n",
      " 98%|█████████▊| 495M/507M [01:06<00:01, 11.2MB/s]go-1]<stderr>:\n",
      " 98%|█████████▊| 496M/507M [01:06<00:00, 11.4MB/s]go-1]<stderr>:\n",
      " 98%|█████████▊| 497M/507M [01:06<00:00, 11.1MB/s]go-1]<stderr>:\n",
      " 98%|█████████▊| 498M/507M [01:06<00:00, 11.5MB/s]go-1]<stderr>:\n",
      " 99%|█████████▊| 499M/507M [01:06<00:00, 11.2MB/s]go-1]<stderr>:\n",
      " 99%|█████████▉| 501M/507M [01:06<00:00, 11.5MB/s]go-1]<stderr>:\n",
      " 99%|█████████▉| 502M/507M [01:07<00:00, 11.2MB/s]go-1]<stderr>:\n",
      " 99%|█████████▉| 503M/507M [01:07<00:00, 11.4MB/s]go-1]<stderr>:\n",
      " 99%|█████████▉| 504M/507M [01:07<00:00, 11.2MB/s]go-1]<stderr>:\n",
      "100%|█████████▉| 505M/507M [01:07<00:00, 11.4MB/s]go-1]<stderr>:\n",
      "100%|█████████▉| 506M/507M [01:07<00:00, 11.2MB/s]go-1]<stderr>:\n",
      "100%|██████████| 507M/507M [01:07<00:00, 7.87MB/s]go-1]<stderr>:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:26:00.613: I smdistributed/modelparallel/torch/worker.py:297] Tracing on GPU. If the model parameters do not fit in a single GPU, you can set trace_device to `cpu`.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:26:02.064: C smdistributed/modelparallel/torch/worker.py:109] [0] Hit an exception for 0/0 on thread 0: CUDA error: an illegal memory access was encountered\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:26:02.065: C smdistributed/modelparallel/torch/worker.py:114] [0]   File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 499, in _thread_compute\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:    self.thread_execute_tracing(req)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 306, in thread_execute_tracing\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:    self._exec_trace_on_device(req, device)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:    self.gen.throw(type, value, traceback)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/state_mod.py\", line 393, in fork_torch_rng_state\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:    torch.cuda.set_rng_state(orig_cuda_rng_state)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/random.py\", line 64, in set_rng_state\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:    _lazy_call(cb)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 153, in _lazy_call\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:    callable()\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/random.py\", line 62, in cb\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:    default_generator.set_state(new_state_copy)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:26:02.065: C smdistributed/modelparallel/torch/worker.py:115] [0] Parent exec stack []\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-06-14 00:26:02.065: C smdistributed/modelparallel/torch/worker.py:116] [0] Req <TraceReq::mb:0, requester:0>\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 434, in record_execution_time\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    yield\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/pooling.py\", line 162, in forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    return F.max_pool2d(input, self.kernel_size, self.stride,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/_jit_internal.py\", line 422, in fn\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    return if_false(*args, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\", line 719, in _max_pool2d\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:During handling of the above exception, another exception occurred:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 434, in record_execution_time\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    yield\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    input = module(input)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1131, in _call_impl\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    return forward_call(*input, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    raise e\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    self.gen.throw(type, value, traceback)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 436, in record_execution_time\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    end.record(current_stream)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    super(Event, self).record(stream)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:RuntimeError: CUDA error: an illegal memory access was encountered[1,mpirank:0,algo-1]<stderr>:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:During handling of the above exception, another exception occurred:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 434, in record_execution_time\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    yield\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torchvision/models/vgg.py\", line 49, in forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    x = self.features(x)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1131, in _call_impl\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    return forward_call(*input, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 92, in trace_forward_seq\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    return trace_forward(self, *args, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    raise e\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    self.gen.throw(type, value, traceback)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 436, in record_execution_time\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    end.record(current_stream)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    super(Event, self).record(stream)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:RuntimeError: CUDA error: an illegal memory access was encountered[1,mpirank:0,algo-1]<stderr>:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:During handling of the above exception, another exception occurred:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 434, in record_execution_time\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    yield\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/ddp_model.py\", line 439, in forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    return self.module(*args, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1131, in _call_impl\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    return forward_call(*input, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    raise e\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    self.gen.throw(type, value, traceback)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 436, in record_execution_time\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    end.record(current_stream)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    super(Event, self).record(stream)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:During handling of the above exception, another exception occurred:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 434, in record_execution_time\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    yield\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/model.py\", line 1009, in forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    return self.module(*args, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1131, in _call_impl\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    return forward_call(*input, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    raise e\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    self.gen.throw(type, value, traceback)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 436, in record_execution_time\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    end.record(current_stream)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    super(Event, self).record(stream)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:During handling of the above exception, another exception occurred:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/state_mod.py\", line 388, in fork_torch_rng_state\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    yield\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 306, in thread_execute_tracing\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    self._exec_trace_on_device(req, device)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 265, in _exec_trace_on_device\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    outputs = step_fn(*args, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"main.py\", line 72, in smp_train_step\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    output = model(data)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1131, in _call_impl\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    return forward_call(*input, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    raise e\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    output = original_forward(self, *args, **kwargs)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    self.gen.throw(type, value, traceback)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 436, in record_execution_time\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    end.record(current_stream)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    super(Event, self).record(stream)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:During handling of the above exception, another exception occurred:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    return _run_code(code, main_globals, None,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    exec(code, run_globals)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    main()\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    run_command_line(args)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    run_path(sys.argv[0], run_name='__main__')\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    return _run_module_code(code, init_globals, run_name,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    _run_code(code, mod_globals, init_globals,\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    exec(code, run_globals)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"main.py\", line 474, in <module>\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    main()\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"main.py\", line 451, in main\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    train(args, model, device, train_loader, optimizer, epoch, scaler)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"main.py\", line 104, in train\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    output, loss = smp_train_step(args, model, data, target, scaler)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 262, in __call__\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    state.exec_server.run_step_leader(mb_args, mb_kwargs, self.id)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/server.py\", line 348, in run_step_leader\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    self.execute_request(\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/server.py\", line 119, in execute_request\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    chosen_worker.execute(req)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 154, in execute\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    self._resume_thread_common()\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 185, in _resume_thread_common\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    self._check_queue_after_thread_return()\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 120, in _check_queue_after_thread_return\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    self._check_exception()\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 117, in _check_exception\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    raise self.exception\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 499, in _thread_compute\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    self.thread_execute_tracing(req)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 306, in thread_execute_tracing\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    self._exec_trace_on_device(req, device)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    self.gen.throw(type, value, traceback)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/state_mod.py\", line 393, in fork_torch_rng_state\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    torch.cuda.set_rng_state(orig_cuda_rng_state)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/random.py\", line 64, in set_rng_state\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    _lazy_call(cb)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 153, in _lazy_call\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    callable()\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/random.py\", line 62, in cb\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:    default_generator.set_state(new_state_copy)\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m [1,mpirank:0,algo-1]<stderr>:RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m --------------------------------------------------------------------------\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m Primary job  terminated normally, but 1 process returned\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m a non-zero exit code. Per user-direction, the job has been aborted.\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m --------------------------------------------------------------------------\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m --------------------------------------------------------------------------\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m mpirun.real detected that one or more processes exited with non-zero status, thus causing\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m the job to be terminated. The first process to do so was:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m \n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m   Process name: [[46271,1],0]\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m   Exit code:    1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m --------------------------------------------------------------------------\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m 2022-06-14 00:26:04,471 sagemaker-training-toolkit ERROR    Reporting training FAILURE\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m 2022-06-14 00:26:04,471 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m ExitCode 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m ErrorMessage \":RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m  : :During handling of the above exception, another exception occurred: : :Traceback (most recent call last): :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 434, in record_execution_time :    yield :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward :    output = original_forward(self, *args, **kwargs) :  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 141, in forward :    input = module(input) :  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1131, in _call_impl :    return forward_call(*input, **kwargs) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward :    raise e :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward :    output = original_forward(self, *args, **kwargs) :  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__ :    self.gen.throw(type, value, traceback) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 436, in record_execution_time :    end.record(current_stream) :  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record :    super(Event, self).record(stream) :RuntimeError: CUDA error: an illegal memory access was encountered: : :During handling of the above exception, another exception occurred: : :Traceback (most recent call last): :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 434, in record_execution_time :    yield :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward :    output = original_forward(self, *args, **kwargs) :  File \"/opt/conda/lib/python3.8/site-packages/torchvision/models/vgg.py\", line 49, in forward :    x = self.features(x) :  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1131, in _call_impl :    return forward_call(*input, **kwargs) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 92, in trace_forward_seq :    return trace_forward(self, *args, **kwargs) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward :    raise e :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward :    output = original_forward(self, *args, **kwargs) :  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__ :    self.gen.throw(type, value, traceback) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 436, in record_execution_time :    end.record(current_stream) :  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record :    super(Event, self).record(stream) :RuntimeError: CUDA error: an illegal memory access was encountered: : :During handling of the above exception, another exception occurred: : :Traceback (most recent call last): :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 434, in record_execution_time :    yield :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward :    output = original_forward(self, *args, **kwargs) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/ddp_model.py\", line 439, in forward :    return self.module(*args, **kwargs) :  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1131, in _call_impl :    return forward_call(*input, **kwargs) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward :    raise e :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward :    output = original_forward(self, *args, **kwargs) :  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__ :    self.gen.throw(type, value, traceback) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 436, in record_execution_time :    end.record(current_stream) :  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record :    super(Event, self).record(stream) :RuntimeError: CUDA error: an illegal memory access was encountered : :During handling of the above exception, another exception occurred: : :Traceback (most recent call last): :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 434, in record_execution_time :    yield :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward :    output = original_forward(self, *args, **kwargs) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/model.py\", line 1009, in forward :    return self.module(*args, **kwargs) :  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1131, in _call_impl :    return forward_call(*input, **kwargs) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward :    raise e :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward :    output = original_forward(self, *args, **kwargs) :  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__ :    self.gen.throw(type, value, traceback) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 436, in record_execution_time :    end.record(current_stream) :  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record :    super(Event, self).record(stream) :RuntimeError: CUDA error: an illegal memory access was encountered : :During handling of the above exception, another exception occurred: : :Traceback (most recent call last): :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/state_mod.py\", line 388, in fork_torch_rng_state :    yield :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 306, in thread_execute_tracing :    self._exec_trace_on_device(req, device) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 265, in _exec_trace_on_device :    outputs = step_fn(*args, **kwargs) :  File \"main.py\", line 72, in smp_train_step :    output = model(data) :  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1131, in _call_impl :    return forward_call(*input, **kwargs) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 75, in trace_forward :    raise e :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/patches/tracing.py\", line 56, in trace_forward :    output = original_forward(self, *args, **kwargs) :  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__ :    self.gen.throw(type, value, traceback) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/module_manager.py\", line 436, in record_execution_time :    end.record(current_stream) :  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/streams.py\", line 176, in record :    super(Event, self).record(stream) :RuntimeError: CUDA error: an illegal memory access was encountered : :During handling of the above exception, another exception occurred: : :Traceback (most recent call last): :  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main :    return _run_code(code, main_globals, None, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module> :    main() :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main :    run_command_line(args) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line :    run_path(sys.argv[0], run_name='__main__') :  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path :    return _run_module_code(code, init_globals, run_name, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code :    _run_code(code, mod_globals, init_globals, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"main.py\", line 474, in <module> :    main() :  File \"main.py\", line 451, in main :    train(args, model, device, train_loader, optimizer, epoch, scaler) :  File \"main.py\", line 104, in train :    output, loss = smp_train_step(args, model, data, target, scaler) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 262, in __call__ :    state.exec_server.run_step_leader(mb_args, mb_kwargs, self.id) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/server.py\", line 348, in run_step_leader :    self.execute_request( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/server.py\", line 119, in execute_request :    chosen_worker.execute(req) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 154, in execute :    self._resume_thread_common() :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 185, in _resume_thread_common :    self._check_queue_after_thread_return() :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 120, in _check_queue_after_thread_return :    self._check_exception() :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 117, in _check_exception :    raise self.exception :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 499, in _thread_compute :    self.thread_execute_tracing(req) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/worker.py\", line 306, in thread_execute_tracing :    self._exec_trace_on_device(req, device) :  File \"/opt/conda/lib/python3.8/contextlib.py\", line 131, in __exit__ :    self.gen.throw(type, value, traceback) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/state_mod.py\", line 393, in fork_torch_rng_state :    torch.cuda.set_rng_state(orig_cuda_rng_state) :  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/random.py\", line 64, in set_rng_state :    _lazy_call(cb) :  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 153, in _lazy_call :    callable() :  File \"/opt/conda/lib/python3.8/site-packages/torch/cuda/random.py\", line 62, in cb :    default_generator.set_state(new_state_copy) :RuntimeError: CUDA error: an illegal memory access was encountered -------------------------------------------------------------------------- Primary job  terminated normally, but 1 process returned a non-zero exit code. Per user-direction, the job has been aborted. mpirun.real detected that one or more processes exited with non-zero status, thus causing the job to be terminated. The first process to do so was:    Process name: [[46271,1],0]   Exit code:    1\"\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m Command \"mpirun --host algo-1-1c6hk:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAINING -x SM_HP_EPOCHS -x SM_HP_BATCH_SIZE -x SM_HP_IMG_SIZE -x SM_HP_DDP -x SM_HP_SMP -x SM_HP_PRESCALED_BATCH -x SM_HP_MP_PARAMETERS -x PYTHONPATH /opt/conda/bin/python3.8 -m mpi4py main.py --batch_size 1 --ddp True --epochs 1 --img_size 6000 --mp_parameters auto_partition=True,ddp=True,fp16_params=True,microbatches=1,optimize=memory,partitions=4,prescaled_batch=False --prescaled_batch False --smp True\"\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk |\u001b[0m 2022-06-14 00:26:04,471 sagemaker-training-toolkit ERROR    Encountered exit_code 1\n",
      "\u001b[36mppsbyzj1gc-algo-1-1c6hk exited with code 1\n",
      "\u001b[0m1\n",
      "Aborting on container exit...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run: ['docker-compose', '-f', '/tmp/tmpfc0xng_9/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, environment, job_name)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0m_stream_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexit_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Process exited with code: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Process exited with code: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_112062/2119559528.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mjob_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"training-job-{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m estimator.fit({'training': s3_data_path},\n\u001b[0m\u001b[1;32m      4\u001b[0m               \u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1806\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1808\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intercept_create_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   4201\u001b[0m             \u001b[0mfunc_name\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mneeded\u001b[0m \u001b[0mintercepting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4202\u001b[0m         \"\"\"\n\u001b[0;32m-> 4203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intercept_create_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, Environment, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HyperParameters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"HyperParameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         training_job.start(\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, environment, job_name)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         self.model_artifacts = self.container.train(\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, environment, job_name)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;31m# which contains the exit code and append the command line to it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Failed to run: %s, %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcompose_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0martifacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompose_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run: ['docker-compose', '-f', '/tmp/tmpfc0xng_9/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1"
     ]
    }
   ],
   "source": [
    "job_name = \"training-job-{}\".format(int(time.time()))\n",
    "\n",
    "estimator.fit({'training': s3_data_path},\n",
    "              job_name=job_name,\n",
    "              wait=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.logs_for_job(job_name=job_name, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
